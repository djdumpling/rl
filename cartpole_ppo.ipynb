{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Tumbling Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Not adding type annotations to my dataclass**. This makes the class-level default values rather than instance attributes that can be set via the constructor.\n",
    "2. **Mixing up standard deviations between actor & critic network**. The critic network needs a larger std (e.g. 1) to estimate returns over a widge range. The actor network needs a smaller std (e.g. 0.01) to make the policy more uniform at the beginning, which encourages exploration instead of action commitment. A small std for the actor network is *one of the most important initialisation details*.\n",
    "3. **Why we use np.empty((0, self.num_envs, ...))**. Also the shape doesn't make intuitive sense, this just essentially preps the experiences to concatenated on via `np.concatenate`. This avoids the awkward case of having the initalization step be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import warnings\n",
    "import time\n",
    "import einops\n",
    "import itertools\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Literal\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Arr: np.ndarray\n",
    "\n",
    "device = t.device(\"mps\") if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else t.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effectively @dataclass is always going to be paired with the Args class since it initalizes all of the arguments\n",
    "# and gets rid of the the annoying initalizations in def __init__(self, )\n",
    "@dataclass \n",
    "class PPOArgs:\n",
    "    seed: int = 1\n",
    "    env_id: str = \"CartPole-PPO\"\n",
    "    mode: Literal[\"classic-control\", \"atari\", \"mujoco\"] = \"classic-control\"\n",
    "\n",
    "    total_timesteps: int = 500000\n",
    "    num_envs: int = 4\n",
    "    num_steps_per_rollout: int = 128\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 4\n",
    "\n",
    "    lr: float = 2.5e-4\n",
    "    max_grad_norm: float = 0.5\n",
    "\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.25\n",
    "\n",
    "    video_log_freq: int | None = None\n",
    "    wandb_project_name: str = \"Cartpole_PPO\"\n",
    "    wandb_entity: str = None\n",
    "\n",
    "    # comments in reference to num_minibatches = 2\n",
    "    def __post_init__(self):\n",
    "        self.batch_size = self.num_steps_per_rollout * self.num_envs # 512\n",
    "\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches # 256 \n",
    "        self.total_phases = self.total_timesteps // self.batch_size # 976\n",
    "        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches # 7808\n",
    "\n",
    "        self.video_save_path = \"videos/cartpole_ppo\"\n",
    "\n",
    "args = PPOArgs(num_minibatches = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is essentially He initialization\n",
    "# orthogonality preserves magnitude and structures of signals as they propagate through the nn\n",
    "def layer_init(layer, std = np.sqrt(2), bias_const = 0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "def get_actor_and_critic_classic(num_obs, num_actions):\n",
    "    actor = nn.Sequential(\n",
    "        layer_init(nn.Linear(num_obs, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, num_actions), std = 0.01),\n",
    "    )\n",
    "\n",
    "    critic = nn.Sequential(\n",
    "        layer_init(nn.Linear(num_obs, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 1), std = 1)\n",
    "    )\n",
    "\n",
    "    return actor, critic\n",
    "\n",
    "# returns the networks used for PPO\n",
    "# comments based on classic-control, atari, mujoco\n",
    "def get_actor_and_critic(envs, mode):\n",
    "    # (4, ), (84, 84, 4), (24, )\n",
    "    obs_shape = envs.single_observation_space.shape\n",
    "    # 4, 28224, 24\n",
    "    num_obs = np.array(obs_shape).prod()\n",
    "    # 2, 4, idk for mujuco yet\n",
    "    num_actions = (envs.single_action_space.n if isinstance(envs.single_action_space, gym.spaces.Discrete)\n",
    "                   else np.array(envs.single_action_space.shape).prod())\n",
    "    \n",
    "    if mode == \"classic_control\":\n",
    "        actor, critic = get_actor_and_critic_classic(num_obs, num_actions)\n",
    "    if mode == \"atari\":\n",
    "        raise NotImplementedError()\n",
    "    if mode == \"mujoco\":\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is GAE (Generalized Advantage Estimation)?**\n",
    "\n",
    "We care about the **advantage function** $A_\\theta(s_t, a_t)$, defined by $Q_\\theta(s_t, a_t) - V_\\theta(s_t)$ which represents the difference. between the expected future reward when taking action $a$ and the reward from taking the expected action according to policy $\\pi_\\theta$ from that point onwards.\n",
    "\n",
    "We start with the 1-step residual $\\hat{A}_\\theta(s_t, a_t) = \\delta_t = r_t + \\gamma \\cdot (1 - d_{t+1}) \\cdot V_\\theta(s_{t+1}) - V_\\theta(s_t)$, but this is too short-sighted. By combining the idea of summing future residuals and adding a geometrically decay factor to account for future residuals, we get $$\\hat{A}_t^{GAE(\\lambda)} = \\delta_t + (\\gamma \\lambda) \\cdot \\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1} \\delta_{T-1}$$\n",
    "\n",
    "We can get the recursive step $\\hat{A}_t^{GAE(\\lambda)} = \\delta_t + (1-d_{t+1}) \\cdot (\\gamma \\lambda) \\cdot \\hat{A}_{t+1}^{GAE(\\lambda)}$ which allows us to start from the final step and work forwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(next_value, next_terminated, rewards, values, terminated, gamma, gae_lambda):\n",
    "    # standardize as floats, also needed for the 1.0 - next_terminated\n",
    "    terminated = terminated.float()\n",
    "    next_terminated = next_terminated.float()\n",
    "\n",
    "    # not sure\n",
    "    next_values = t.concat(values[1:], next_value[None, :])\n",
    "    next_terminated = t.concat(terminated[1:], next_terminated[None, :])\n",
    "\n",
    "    # calculated from above\n",
    "    deltas = rewards + gamma * (1.0 - next_terminated) * next_values - values\n",
    "    advantages = t.zeros_like(deltas)\n",
    "    \n",
    "    # update from the last element\n",
    "    advantages[-1] = deltas[-1]\n",
    "\n",
    "    # recursively iterate from the back\n",
    "    for s in reversed(range(values.shape[0] - 1)):\n",
    "        advantages[s] = rewards[s] + (1 - next_terminated[s]) * gamma * gae_lambda * advantages[s + 1]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have `num_envs` vectorized environments and `num_steps_per_rollout`, then we have a total `batch_size = num_envs * num_steps_per_rollout`. Our goal is to randomly split those `batch_size` agent actions into minibatches with size `minibatch_size.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch_indices(rng, batch_size, minibatch_size):\n",
    "    num_minibatches = batch_size / minibatch_size\n",
    "\n",
    "    # numbers 0 -> batch_size-1, shaped by the num_minibatches\n",
    "    indices = rng.permutation(batch_size).reshape(num_minibatches, minibatch_size)\n",
    "    return list(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure why we switch between Tensor and np so often\n",
    "# keeps all per-timestep rollout data neatly packaged, easier function interfaces\n",
    "@dataclass\n",
    "class ReplayMinibatch:\n",
    "    obs: Float[Tensor, \"minibatch_size *obs_shape\"]\n",
    "    actions: Int[Tensor, \"minibatch_size *action_shape\"]\n",
    "    logprobs: Float[Tensor, \"minibatch_size\"]\n",
    "    advantages: Float[Tensor, \"minibatch_size\"]\n",
    "    returns: Float[Tensor, \"minibatch_size\"]\n",
    "    terminated: Bool[Tensor, \"minibatch_size\"]\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, num_envs, obs_shape, action_shape, batch_size, minibatch_size, batches_per_learning_phase, seed: int = 42):\n",
    "        self.num_envs = num_envs\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.batches_per_learning_phase = batches_per_learning_phase\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.reset()\n",
    "\n",
    "    # *self.obs_shape and *self.action_shape allows for shape flexibility\n",
    "    def reset(self):\n",
    "        self.obs = np.empty((0, self.num_envs, *self.obs_shape), dtype=np.float32)\n",
    "        self.actions = np.empty((0, self.num_envs, *self.action_shape), dtype=np.int32)\n",
    "        self.logprobs = np.empty((0, self.num_envs), dtype=np.float32)\n",
    "        self.values = np.empty((0, self.num_envs), dtype=np.float32)\n",
    "        self.rewards = np.empty((0, self.num_envs), dtype=np.float32)\n",
    "        self.terminated = np.empty((0, self.num_envs), dtype=bool)\n",
    "\n",
    "    # using [None, :] adds another axis at the beginning, which is probably used as a time-step\n",
    "    def add(self, obs, actions, logprobs, values, rewards, terminated):\n",
    "        self.obs = np.concatenate((self.obs, obs[None, :]))\n",
    "        self.actions = np.concatenate((self.actions, actions[None, :]))\n",
    "        self.logprobs = np.concatenate((self.logprobs, logprobs[None, :]))\n",
    "        self.values = np.concatenate((self.values, values[None, :]))\n",
    "        self.rewards = np.concatenate((self.rewards, rewards[None, :]))\n",
    "        self.terminated = np.concatenate((self.terminated, terminated[None, :]))\n",
    "\n",
    "    def get_minibatches(self, next_value, next_terminated, gamma, gae_lambda):\n",
    "        obs, actions, logprobs, values, rewards, terminated = (\n",
    "            t.tensor(x, device = device, dtype = t.float32)\n",
    "            for x in [self.obs, self.actions, self.logprobs, self.values, self.rewards, self.terminated]\n",
    "        )\n",
    "\n",
    "        # these two will be useful in the critic learning value function, which depends on V_target, aka returns\n",
    "        advantages = compute_advantages(next_value, next_terminated, rewards, values, terminated, gamma, gae_lambda)\n",
    "        returns = advantages + values\n",
    "\n",
    "        minibatches = []\n",
    "        # we update on each experience more than once, particularly self.batches_per_learning_phase times\n",
    "        for _ in range(self.batches_per_learning_phase):\n",
    "            for indices in get_minibatch_indices(self.rng, self.batch_size, self.minibatch_size):\n",
    "                # not sure what the (0,1) means, but should combine the [T, num_envs, feature_dim] -> [T * num_envs, feature_dim]\n",
    "                minibatches.append(\n",
    "                    ReplayMinibatch(*[data.flatten(0, 1)[indices] \n",
    "                                    for data in [obs, actions, logprobs, advantages, returns, terminated]])\n",
    "                )\n",
    "\n",
    "        # clears the rollout buffer for the next training cycle\n",
    "        self.reset()\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, envs, actor, critic, memory):\n",
    "        self.envs = envs\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.memory = memory\n",
    "\n",
    "        self.step = 0\n",
    "        self.next_obs = t.tensor(envs.reset()[0], device = device, dtype = t.float)\n",
    "        self.next_terminated = t.zeros(envs.num_envs, device = device, dtype = t.bool)\n",
    "\n",
    "    def play_step(self):\n",
    "        obs = self.next_obs\n",
    "        terminated = self.next_terminated\n",
    "\n",
    "        # f-prop to get the policy\n",
    "        # why no flatten here? maybe dist can't do it\n",
    "        with t.inference_mode():\n",
    "            logits = self.actor(obs)\n",
    "            values = self.critic(obs).flatten().cpu().numpy()\n",
    "\n",
    "        # sample from the policy to determine which action to take\n",
    "        dist = Categorical(logits = logits)\n",
    "        actions = dist.sample()\n",
    "\n",
    "        # take the action -> step\n",
    "        # not sure what .cpu().numpy() is doing, some device shi\n",
    "        next_obs, rewards, next_terminated, next_truncated, infos = self.envs.step(actions.cpu().numpy())\n",
    "\n",
    "        # add everything into the memory\n",
    "        logprobs = dist.log_prob(actions).cpu().numpy()\n",
    "        self.memory.add(obs.cpu().numpy(), actions.cpu().numpy(), logprobs, values, rewards, terminated.cpu().numpy())\n",
    "\n",
    "        # update info for next step\n",
    "        self.next_obs = t.from_numpy(next_obs).to(device, dtype = t.float)\n",
    "        self.next_terminated = t.from_numpy(next_terminated).to(device, dtype = t.float)\n",
    "        self.step += self.envs.num_envs\n",
    "\n",
    "        return infos\n",
    "    \n",
    "    # not sure why we need .flatten()\n",
    "    def get_minibatches(self, gamma, gae_lambda):\n",
    "        # get the minibatches from the memory\n",
    "        with t.inference_mode():\n",
    "            next_value = self.critic(self.obs).flatten()\n",
    "        minibatches = self.memory.get_minibatches(next_value, self.next_terminated, gamma, gae_lambda)\n",
    "        self.memory.reset()\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate the three components that will comrise the final training objective, given by $$L_t^{PPO}(\\theta) = \\hat{\\mathbb{E}}_t\\left[L_t^{CLIP}(\\theta) - c_1 L_t^{VF}(\\theta) + c_2 S[\\pi_\\theta](s_t)\\right]$$\n",
    "where the first reprents the PPO objective function $L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t\\left[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)\\right]$.\n",
    "\n",
    "The second represents the critic learning value function, defined by $$L^{VF}(\\theta) = (V_\\theta(s_t)-V_t^{target})^2$$ where $V_t^{target} = V_{\\theta_{target}}(s_t)+\\hat{A}_{\\theta_{target}}(s_t, a_t)$ is the return.\n",
    "\n",
    "The third represents the entropy which encourages exploration. The standard formula for a discrete probability distribution $p$ is given by $$H(p) = \\sum_x p(x) \\ln\\left(\\frac1{p(x)}\\right)$$ with the convention that $\\ln\\left(\\frac1{0}\\right)=0$. The larger the entropy, the more uncertain the action is, which essentially encourages exploration. If entropy is 0, there is no certainity and hence no exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_clipped_surrogate_objective(probs, mb_action, mb_advantages, mb_logprobs, clip_coef, eps: float = 1e-8):\n",
    "    # we define prob_ratio as the ratio of the probabilities of new policy to the old policy\n",
    "    #  e^{logx - logy} = x/y, which is what we want\n",
    "    logits_diff = probs.log_prob(mb_action) - mb_logprobs\n",
    "    prob_ratio = t.exp(logits_diff)\n",
    "\n",
    "    # we standardize the mb_advantage\n",
    "    # not sure why we standardize\n",
    "    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n",
    "\n",
    "    # standard application of the formula\n",
    "    # not sure: need to explain why this works\n",
    "    non_clipped = prob_ratio * mb_advantages\n",
    "    clipped = t.clip(prob_ratio, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n",
    "\n",
    "    return t.minimum(non_clipped, clipped).mean()\n",
    "\n",
    "def calc_value_function_loss(values, mb_returns, vf_coef):\n",
    "    return vf_coef * ((values - mb_returns)**2).mean()\n",
    "\n",
    "def calc_entropy_bonus(dist, ent_coef):\n",
    "    return ent_coef * dist.entropy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a linear scheduler which decreases the learning rate from start (`'initial_lr`) to end (`end_lr`). If we wanted it to decay for some period then stay constant afterwards, we would do something like in `cartpole_dqn.ipynb` and use the `min()` function, sorta like a ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOScheduler:\n",
    "    def __init__(self, optimizer, initial_lr, end_lr, total_phases):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.total_phases = total_phases\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    # the scheduler than linearly decays from initial_lr to end_lr\n",
    "    def step(self):\n",
    "        self.n_step_calls += 1\n",
    "        frac = self.n_step_calls / self.total_phases\n",
    "        # we update for both actor and critic\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = self.initial_lr + frac * (self.end_lr - self.initial_lr)\n",
    "\n",
    "def make_optimizer(actor, critic, total_phases, initial_lr, end_lr):\n",
    "    # passes all parameters (actor and critic) into a single optimizer\n",
    "    optimizer = optim.AdamW(itertools.chain(\n",
    "        actor.parameters(), critic.parameters()), lr = initial_lr, eps = 1e-5, maximize = True)\n",
    "    scheduler = PPOScheduler(optimizer, initial_lr, end_lr, total_phases)\n",
    "    return optimizer, scheduler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
