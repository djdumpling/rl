{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "668b0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jaxtyping\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import tabulate\n",
    "from eindex import eindex\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RLHFArgs:\n",
    "    # random\n",
    "    seed: int = 1\n",
    "\n",
    "    # logging\n",
    "    wandb_project_name: str = \"rlhf_transformers\"\n",
    "    wandb_entity: str | None = None\n",
    "\n",
    "    # macro-training \n",
    "    total_phases: int = 200\n",
    "    batch_size: int = 32 #enforce batch_size % num_minibatches == 0\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 2\n",
    "\n",
    "    # optimization hyperparameters\n",
    "    base_lr: float = 2e-5\n",
    "    head_lr: float = 5e-4\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 20 #enforce warmup_steps < total_phases\n",
    "    final_scale: float = 0.1\n",
    "\n",
    "    # PPO objective function coefficients\n",
    "    clip_coef: float = 0.2\n",
    "    vf_coef: float = 0.15\n",
    "    ent_coef: float = 0.001\n",
    "\n",
    "    # model and sampling with prefix\n",
    "    base_model: str = \"gpt-medium\"\n",
    "    gen_len: int = 50\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 10\n",
    "    prefix: str = \"This is\"\n",
    "    prepend_bos: bool = True\n",
    "\n",
    "    # RLHF-specific arguments\n",
    "    kl_coef: float = 2.5\n",
    "    reward_fn: Callable = lambda x: 0.0\n",
    "    normalize_reward: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a83df",
   "metadata": {},
   "source": [
    "# Setup: working with the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e06f9",
   "metadata": {},
   "source": [
    "Right after the last layernorm before we unembed our tokens, we add a hook function (our value head) which computes a **value estimate** for the generated sequence. The hook function is a simple 2-layer neural network which computs the value estimate during the forward pass and stores it externally.\n",
    "\n",
    "Why do we choose this location? After the layernorm essentially normalizes the reward, and before the unembedding because we take in the enumerated tokens as input. It is also towards the end because (supposedly) it contains the most information after accumulating through the residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "class TransformerWithValueHead(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = HookedTransformer.from_pretrained(base_model)\n",
    "        \n",
    "        d_model = self.base_model.cfg.d_model\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, 1))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        value_head_output = None\n",
    "\n",
    "        # resid_post: [batch seq d_model] so\n",
    "        # value_head_ouput: [batch seq]\n",
    "        def calc_and_store_value_head_output(resid_post, hook):\n",
    "            # nonlocal: for variables inside nested functions\n",
    "            nonlocal value_head_output\n",
    "            value_head_output = self.value_head(resid_post).squeeze(-1)\n",
    "\n",
    "        # run_with_hooks injects parameters\n",
    "        logits = self.base_model.run_with_hooks(\n",
    "            input_ids,\n",
    "            return_type = \"logits\",\n",
    "            # \"normalized\" to represent being after the LayerNorm\n",
    "            fwd_hooks = [(utils.get_act_name(\"normalized\"), calc_and_store_value_head_output)])\n",
    "        \n",
    "        return logits, value_head_output\n",
    "    \n",
    "model = TransformerWithValueHead(\"gpt2-small\").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74bc51",
   "metadata": {},
   "source": [
    "Defaulting `stop_at_eos = False` is interesting. From an interpretability perspective, `stop_at_eos = False`  helps with seeing hallucations. From a training perspective, it helps measure how well the model learned to stop and enables models to learn from full length text, not truncated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dddcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend_bos: appending a BOS token at the start of a sequence, which marks the start\n",
    "def get_samples(base_model, prompt, batch_size, gen_len, temperature, top_k, prepend_bos):\n",
    "    # returns one tokenized prompt, squeeze to extract pure tokens\n",
    "    input_ids = base_model.to_tokens(prompt, prepend_bos = prepend_bos).squeeze(0)\n",
    "\n",
    "    output_ids = base_model.generate(\n",
    "        # [tokens] becomes [batch_size tokens]\n",
    "        # repeats input_ids once batch_size times\n",
    "        input_ids.repeat(batch_size, 1), \n",
    "        max_new_tokens = gen_len, \n",
    "        stop_at_eos = False,\n",
    "        temperature = temperature,\n",
    "        top_k = top_k, \n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    # samples: [batch_size sequence]\n",
    "    samples = base_model.to_string(output_ids)\n",
    "\n",
    "    # .clone() to prevent modification to internal output_ids\n",
    "    return output_ids.clone(), samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eb89e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Token IDs                                              </span>┃<span style=\"font-weight: bold\"> Samples                                                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ [1212, 3807, 373, 1107, 1257, 13, 314, 892, 257, 1256, │ 'This movie was really fun. I think a lot of people    │\n",
       "│ 286, 661, 2936, 326, 314, 373, 1804, 340, 2642]        │ felt that I was doing it wrong'                        │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 2089, 13, 198, 198, 1026, 373, │ 'This movie was really bad.\\n\\nIt was actually very    │\n",
       "│ 1682, 845, 2089, 287, 262, 2565, 326, 11, 611]         │ bad in the sense that, if'                             │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 4998, 11, 340, 1107, 7907,     │ 'This movie was really amazing, it really captured our │\n",
       "│ 674, 13843, 290, 2921, 514, 262, 2863, 284, 766, 703]  │ imagination and gave us the chance to see how'         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1257, 11, 290, 314, 6151, 262, │ 'This movie was really fun, and I loved the fact that  │\n",
       "│ 1109, 326, 356, 550, 284, 670, 1107, 1327, 284]        │ we had to work really hard to'                         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1049, 13, 314, 373, 1107,      │ \"This movie was really great. I was really             │\n",
       "│ 11679, 351, 340, 13, 632, 338, 407, 262, 749, 3499]    │ disappointed with it. It's not the most interesting\"   │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mToken IDs                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSamples                                               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ [1212, 3807, 373, 1107, 1257, 13, 314, 892, 257, 1256, │ 'This movie was really fun. I think a lot of people    │\n",
       "│ 286, 661, 2936, 326, 314, 373, 1804, 340, 2642]        │ felt that I was doing it wrong'                        │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 2089, 13, 198, 198, 1026, 373, │ 'This movie was really bad.\\n\\nIt was actually very    │\n",
       "│ 1682, 845, 2089, 287, 262, 2565, 326, 11, 611]         │ bad in the sense that, if'                             │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 4998, 11, 340, 1107, 7907,     │ 'This movie was really amazing, it really captured our │\n",
       "│ 674, 13843, 290, 2921, 514, 262, 2863, 284, 766, 703]  │ imagination and gave us the chance to see how'         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1257, 11, 290, 314, 6151, 262, │ 'This movie was really fun, and I loved the fact that  │\n",
       "│ 1109, 326, 356, 550, 284, 670, 1107, 1327, 284]        │ we had to work really hard to'                         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1049, 13, 314, 373, 1107,      │ \"This movie was really great. I was really             │\n",
       "│ 11679, 351, 340, 13, 632, 338, 407, 262, 749, 3499]    │ disappointed with it. It's not the most interesting\"   │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ids, samples = get_samples(\n",
    "    model.base_model,\n",
    "    prompt = \"This movie was really\",\n",
    "    batch_size = 5,\n",
    "    gen_len = 15,\n",
    "    temperature = 0.8,\n",
    "    top_k = 15,\n",
    "    prepend_bos = False\n",
    ")\n",
    "\n",
    "table = Table(\"Token IDs\", \"Samples\", show_lines = True)\n",
    "for ids, sample in zip(sample_ids, samples):\n",
    "    # ids.tolist(): convert Tensor into Python list\n",
    "    # repr(sample): printable representation (adds single quotes)\n",
    "    table.add_row(str(ids.tolist()), repr(sample))\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae35443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf405a638f6b43dbbb60fe0e339febc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0875b8023f8d4d268628f7e610683608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e53f745a3849e9ab2f9fb055073c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da49371f562b4c12b64568c892dc08c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f6df81f69e4a24a94966e98b12e917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bb65e540fa462692647e748ed89aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b8c1512a3543c8b79ed2675d98c7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# .half(): uses float16 precision for faster inference on GPUs\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\").half().to(device)\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "def reward_fn_sentiment_imdb(gen_sample, direction):\n",
    "    # \"pt\" for pytorch tensors, padding + truncation to ensure same length generation\n",
    "    tokens = cls_tokenizer(gen_sample, return_tensors = \"pt\", padding = True, truncation = True)[\"input_ids\"].to(device)\n",
    "    # logits: [batch_size, 2] for pos/neg classification\n",
    "    logits = cls_model(tokens).logits\n",
    "    # positive_cls: [batch_size] contains relevant class after softmaxing to get probabilities\n",
    "    positive_cls = logits.softmax(-1)[:, 1 if (direction == \"pos\") else 0]\n",
    "    return positive_cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18147c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_reward(reward, eps = 1e-5):\n",
    "    return (reward - reward.mean()) / (reward.std() + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b53665",
   "metadata": {},
   "source": [
    "Using the simple $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ formula where $Q(s_t, a_t)$ is based off of the one-step Q estimates. if $t<T$, then our Q estimate is $V(s_{t+1})$, but if $t=T$, then we can use the known reward $r_t$ for the entire sequence.\n",
    "\n",
    "GAE is an alternative but wouldn't bring a significant improvement since GAE is most helpful in reducing variance in advantage estimation, and our situation is low variance (each step adds a single token to our sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79c32d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(values, rewards, prefix_len):\n",
    "    one_step_est = t.cat([values[:, prefix_len:-1], rewards[:, None]], dim = -1)\n",
    "    zero_step_est = values[prefix_len-1:-1]\n",
    "    return one_step_est - zero_step_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a52786",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc28b7",
   "metadata": {},
   "source": [
    "Compared to the PPO implementation, there are a few differences in `ReplayMemory`. \n",
    "- Don't need an `add` function because we add it all at once instead of one-by-one.\n",
    "- Don't need multiple environments\n",
    "\n",
    "And for `ReplayMinibatch`\n",
    "- Don't need `actions` anymore since there isn't a sense of an \"agent\" since actions (tokens generated) are contained within the sequences\n",
    "- Don't need `dones` since we set the sequence to be `gen_len` long\n",
    "- Sotre `ref_logits` as a part of the KL penalty w.r.t the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcb2df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ??? figure out why the sizing is what it is\n",
    "class ReplayMinibatch:\n",
    "    sample_ids: Float[Tensor, \"minibatch_size seq_len\"]\n",
    "    logprobs: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    advantages: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    returns: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"]\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, args, sample_ids, logprobs, advantages, values, ref_logits):\n",
    "        self.args = args\n",
    "        self.sample_ids = sample_ids\n",
    "        self.logprobs = logprobs\n",
    "        self.advantages = advantages\n",
    "        self.values = values\n",
    "        self.ref_logits = ref_logits\n",
    "\n",
    "    def get_minibatches(self):\n",
    "        minibatches = []\n",
    "\n",
    "        # since we use 1-step advantage estimation\n",
    "        # returns = next-step estimate of value function\n",
    "        returns = self.advantages + self.values[:, -self.args.gen_len - 1: -1]\n",
    "\n",
    "        for _ in range(self.args.batches_per_learning_phase):\n",
    "            for indices in t.randperm(self.args.batch_size).reshape(self.args.num_minibatches, -1):\n",
    "                minibatches.append(ReplayMinibatch(\n",
    "                    sample_ids = self.sample_ids[indices],\n",
    "                    logprobs=self.logprobs[indices],\n",
    "                    advantages=self.advantages[indices],\n",
    "                    returns=returns[indices],\n",
    "                    ref_logits=self.ref_logits[indices]\n",
    "                ))\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb43bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't actually need gen_len here\n",
    "def calc_kl_penalty(logits, ref_logits, kl_coef, gen_len):\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    ref_log_probs = ref_logits.log_softmax(-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    kl_div = (probs * (log_probs - ref_log_probs)).sum(-1)\n",
    "\n",
    "    return kl_coef * kl_div.mean()\n",
    "\n",
    "def calc_entropy_bonus(logits, ent_coef, gen_len):\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    entropy = -(log_probs * probs).sum(-1)\n",
    "\n",
    "    return ent_coef * entropy.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
