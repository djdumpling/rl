{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the over-annotated code, it's more brain-dumping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Tumbling Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Not adding type annotations to my dataclass**. This makes the class-level default values rather than instance attributes that can be set via the constructor.\n",
    "2. **Mixing up standard deviations between actor & critic network**. The critic network needs a larger std (e.g. 1) to estimate returns over a widge range. The actor network needs a smaller std (e.g. 0.01) to make the policy more uniform at the beginning, which encourages exploration instead of action commitment. A small std for the actor network is *one of the most important initialisation details*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import warnings\n",
    "import time\n",
    "import einops\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Literal\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Arr: np.ndarray\n",
    "\n",
    "device = t.device(\"mps\") if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else t.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effectively @dataclass is always going to be paired with the Args class since it initalizes all of the arguments\n",
    "# and gets rid of the the annoying initalizations in def __init__(self, )\n",
    "@dataclass \n",
    "class PPOArgs:\n",
    "    seed: int = 1\n",
    "    env_id: str = \"CartPole-PPO\"\n",
    "    mode: Literal[\"classic-control\", \"atari\", \"mujoco\"] = \"classic-control\"\n",
    "\n",
    "    total_timesteps: int = 500000\n",
    "    num_envs: int = 4\n",
    "    num_steps_per_rollout: int = 128\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 4\n",
    "\n",
    "    lr: float = 2.5e-4\n",
    "    max_grad_norm: float = 0.5\n",
    "\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.25\n",
    "\n",
    "    video_log_freq: int | None = None\n",
    "    wandb_project_name: str = \"Cartpole_PPO\"\n",
    "    wandb_entity: str = None\n",
    "\n",
    "    # comments in reference to num_minibatches = 2\n",
    "    def __post_init__(self):\n",
    "        self.batch_size = self.num_steps_per_rollout * self.num_envs # 512\n",
    "\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches # 256 \n",
    "        self.total_phases = self.total_timesteps // self.batch_size # 976\n",
    "        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches # 7808\n",
    "\n",
    "        self.video_save_path = \"videos/cartpole_ppo\"\n",
    "\n",
    "args = PPOArgs(num_minibatches = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is essentially He initialization\n",
    "# orthogonality preserves magnitude and structures of signals as they propagate through the nn\n",
    "def layer_init(layer, std = np.sqrt(2), bias_const = 0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "def get_actor_and_critic_classic(num_obs, num_actions):\n",
    "    actor = nn.Sequential(\n",
    "        layer_init(nn.Linear(num_obs, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, num_actions), std = 0.01),\n",
    "    )\n",
    "\n",
    "    critic = nn.Sequential(\n",
    "        layer_init(nn.Linear(num_obs, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 1), std = 1)\n",
    "    )\n",
    "\n",
    "    return actor, critic\n",
    "\n",
    "# returns the networks used for PPO\n",
    "# comments based on classic-control, atari, mujoco\n",
    "def get_actor_and_critic(envs, mode):\n",
    "    # (4, ), (84, 84, 4), (24, )\n",
    "    obs_shape = envs.single_observation_space.shape\n",
    "    # 4, 28224, 24\n",
    "    num_obs = np.array(obs_shape).prod()\n",
    "    # 2, 4, idk for mujuco yet\n",
    "    num_actions = (envs.single_action_space.n if isinstance(envs.single_action_space, gym.spaces.Discrete)\n",
    "                   else np.array(envs.single_action_space.shape).prod())\n",
    "    \n",
    "    if mode == \"classic_control\":\n",
    "        actor, critic = get_actor_and_critic_classic(num_obs, num_actions)\n",
    "    if mode == \"atari\":\n",
    "        raise NotImplementedError()\n",
    "    if mode == \"mujoco\":\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(next_value, next_terminated, rewards, values, terminated, gamma, gae_lambda):\n",
    "    terminated = terminated.float()\n",
    "    next_terminated = next_terminated.float()\n",
    "\n",
    "    next_values = t.concat(values[1:], next_value[None, :])\n",
    "    next_terminated = t.concat(terminated[1:], next_terminated[None, :])\n",
    "\n",
    "    deltas = rewards + gamma * (1.0 - next_terminated) * next_values - values\n",
    "    advantages = t.zeros_like(deltas)\n",
    "    \n",
    "    advantages[-1] = deltas[-1]\n",
    "\n",
    "    for s in reversed(range(values.shape[0] - 1)):\n",
    "        advantages[s] = rewards[s] + (1 - next_terminated[s]) * gamma * gae_lambda * advantages[s + 1]\n",
    "\n",
    "    return advantages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
