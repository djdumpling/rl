{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "668b0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jaxtyping\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import tabulate\n",
    "from eindex import eindex\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fix tokenizers warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RLHFArgs:\n",
    "    # random\n",
    "    seed: int = 1\n",
    "\n",
    "    # logging\n",
    "    wandb_project_name: str = \"rlhf_transformers\"\n",
    "    wandb_entity: str | None = None\n",
    "\n",
    "    # macro-training \n",
    "    total_phases: int = 200\n",
    "    batch_size: int = 32 #enforce batch_size % num_minibatches == 0\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 2\n",
    "\n",
    "    # optimization hyperparameters\n",
    "    base_lr: float = 2e-5\n",
    "    head_lr: float = 5e-4\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 20 #enforce warmup_steps < total_phases\n",
    "    final_scale: float = 0.1\n",
    "\n",
    "    # PPO objective function coefficients\n",
    "    clip_coef: float = 0.2\n",
    "    vf_coef: float = 0.15\n",
    "    ent_coef: float = 0.001\n",
    "\n",
    "    # model and sampling with prefix\n",
    "    base_model: str = \"gpt2-small\"\n",
    "    gen_len: int = 50\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 10\n",
    "    prefix: str = \"This movie was really\"\n",
    "    prepend_bos: bool = True\n",
    "\n",
    "    # RLHF-specific arguments\n",
    "    kl_coef: float = 2.5\n",
    "    reward_fn: Callable = lambda x: 0.0\n",
    "    normalize_reward: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a83df",
   "metadata": {},
   "source": [
    "# Setup: working with the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e06f9",
   "metadata": {},
   "source": [
    "Right after the last layernorm before we unembed our tokens, we add a hook function (our value head) which computes a **value estimate** for the generated sequence. The hook function is a simple 2-layer neural network which computs the value estimate during the forward pass and stores it externally.\n",
    "\n",
    "Why do we choose this location? After the layernorm essentially normalizes the reward, and before the unembedding because we take in the enumerated tokens as input. It is also towards the end because (supposedly) it contains the most information after accumulating through the residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d11d5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "class TransformerWithValueHead(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = HookedTransformer.from_pretrained(base_model)\n",
    "        \n",
    "        d_model = self.base_model.cfg.d_model\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, 1))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        value_head_output = None\n",
    "\n",
    "        # resid_post: [batch seq d_model] so\n",
    "        # value_head_ouput: [batch seq]\n",
    "        def calc_and_store_value_head_output(resid_post, hook):\n",
    "            # nonlocal: for variables inside nested functions\n",
    "            nonlocal value_head_output\n",
    "            value_head_output = self.value_head(resid_post).squeeze(-1)\n",
    "\n",
    "        # run_with_hooks injects parameters\n",
    "        logits = self.base_model.run_with_hooks(\n",
    "            input_ids,\n",
    "            return_type = \"logits\",\n",
    "            # \"normalized\" to represent being after the LayerNorm\n",
    "            fwd_hooks = [(utils.get_act_name(\"normalized\"), calc_and_store_value_head_output)])\n",
    "        \n",
    "        return logits, value_head_output\n",
    "    \n",
    "model = TransformerWithValueHead(\"gpt2-small\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74bc51",
   "metadata": {},
   "source": [
    "Defaulting `stop_at_eos = False` is interesting. From an interpretability perspective, `stop_at_eos = False`  helps with seeing hallucations. From a training perspective, it helps measure how well the model learned to stop and enables models to learn from full length text, not truncated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dddcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend_bos: appending a BOS token at the start of a sequence, which marks the start\n",
    "def get_samples(base_model, prompt, batch_size, gen_len, temperature, top_k, prepend_bos):\n",
    "    # returns one tokenized prompt, squeeze to extract pure tokens\n",
    "    input_ids = base_model.to_tokens(prompt, prepend_bos = prepend_bos).squeeze(0)\n",
    "\n",
    "    output_ids = base_model.generate(\n",
    "        # [tokens] becomes [batch_size tokens]\n",
    "        # repeats input_ids once batch_size times\n",
    "        input_ids.repeat(batch_size, 1), \n",
    "        max_new_tokens = gen_len, \n",
    "        stop_at_eos = False,\n",
    "        temperature = temperature,\n",
    "        top_k = top_k, \n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    # samples: [batch_size sequence]\n",
    "    samples = base_model.to_string(output_ids)\n",
    "\n",
    "    # .clone() to prevent modification to internal output_ids\n",
    "    return output_ids.clone(), samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3eb89e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Token IDs                                              </span>┃<span style=\"font-weight: bold\"> Samples                                                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ [1212, 3807, 373, 1107, 1049, 13, 314, 6151, 340, 11,  │ 'This movie was really great. I loved it, and I would  │\n",
       "│ 290, 314, 561, 2342, 340, 757, 290, 757, 13]           │ watch it again and again.'                             │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 880, 3194, 290, 2826, 21104,   │ 'This movie was really well written and played         │\n",
       "│ 13, 198, 198, 40, 635, 4047, 4313, 3555, 262, 1492]    │ beautifully.\\n\\nI also highly recommend reading the    │\n",
       "│                                                        │ book'                                                  │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 11, 1107, 922, 11, 290, 326,   │ \"This movie was really, really good, and that is what  │\n",
       "│ 318, 644, 314, 892, 340, 338, 546, 13, 632]            │ I think it's about. It\"                                │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 655, 257, 7684, 286, 1257,     │ 'This movie was really just a bunch of fun kids        │\n",
       "│ 3988, 1972, 511, 2832, 319, 617, 7165, 3404, 290, 314] │ getting their hands on some crazy stuff and I'         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 8258, 290, 845, 880, 925, 13,  │ \"This movie was really funny and very well made. It's  │\n",
       "│ 632, 338, 281, 3499, 1621, 351, 6088, 286, 30953]      │ an interesting story with plenty of twists\"            │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mToken IDs                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSamples                                               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ [1212, 3807, 373, 1107, 1049, 13, 314, 6151, 340, 11,  │ 'This movie was really great. I loved it, and I would  │\n",
       "│ 290, 314, 561, 2342, 340, 757, 290, 757, 13]           │ watch it again and again.'                             │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 880, 3194, 290, 2826, 21104,   │ 'This movie was really well written and played         │\n",
       "│ 13, 198, 198, 40, 635, 4047, 4313, 3555, 262, 1492]    │ beautifully.\\n\\nI also highly recommend reading the    │\n",
       "│                                                        │ book'                                                  │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 11, 1107, 922, 11, 290, 326,   │ \"This movie was really, really good, and that is what  │\n",
       "│ 318, 644, 314, 892, 340, 338, 546, 13, 632]            │ I think it's about. It\"                                │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 655, 257, 7684, 286, 1257,     │ 'This movie was really just a bunch of fun kids        │\n",
       "│ 3988, 1972, 511, 2832, 319, 617, 7165, 3404, 290, 314] │ getting their hands on some crazy stuff and I'         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 8258, 290, 845, 880, 925, 13,  │ \"This movie was really funny and very well made. It's  │\n",
       "│ 632, 338, 281, 3499, 1621, 351, 6088, 286, 30953]      │ an interesting story with plenty of twists\"            │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ids, samples = get_samples(\n",
    "    model.base_model,\n",
    "    prompt = \"This movie was really\",\n",
    "    batch_size = 5,\n",
    "    gen_len = 15,\n",
    "    temperature = 0.8,\n",
    "    top_k = 15,\n",
    "    prepend_bos = False\n",
    ")\n",
    "\n",
    "table = Table(\"Token IDs\", \"Samples\", show_lines = True)\n",
    "for ids, sample in zip(sample_ids, samples):\n",
    "    # ids.tolist(): convert Tensor into Python list\n",
    "    # repr(sample): printable representation (adds single quotes)\n",
    "    table.add_row(str(ids.tolist()), repr(sample))\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ae35443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .half(): uses float16 precision for faster inference on GPUs\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\").half().to(device)\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "def reward_fn_sentiment_imdb(gen_sample, direction: str = \"pos\"):\n",
    "    # \"pt\" for pytorch tensors, padding + truncation to ensure same length generation\n",
    "    tokens = cls_tokenizer(gen_sample, return_tensors = \"pt\", padding = True, truncation = True)[\"input_ids\"].to(device)\n",
    "    # logits: [batch_size, 2] for pos/neg classification\n",
    "    logits = cls_model(tokens).logits\n",
    "    # positive_cls: [batch_size] contains relevant class after softmaxing to get probabilities\n",
    "    positive_cls = logits.softmax(-1)[:, 1 if (direction == \"pos\") else 0]\n",
    "    return positive_cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18147c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_reward(reward, eps = 1e-5):\n",
    "    return (reward - reward.mean()) / (reward.std() + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b53665",
   "metadata": {},
   "source": [
    "Using the simple $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ formula where $Q(s_t, a_t)$ is based off of the one-step Q estimates. if $t<T$, then our Q estimate is $V(s_{t+1})$, but if $t=T$, then we can use the known reward $r_t$ for the entire sequence.\n",
    "\n",
    "GAE is an alternative but wouldn't bring a significant improvement since GAE is most helpful in reducing variance in advantage estimation, and our situation is low variance (each step adds a single token to our sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79c32d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(values, rewards, prefix_len):\n",
    "    one_step_est = t.cat([values[:, prefix_len:-1], rewards[:, None]], dim = -1)\n",
    "    zero_step_est = values[:,  prefix_len-1:-1]\n",
    "    return one_step_est - zero_step_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a52786",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc28b7",
   "metadata": {},
   "source": [
    "Compared to the PPO implementation, there are a few differences in `ReplayMemory`. \n",
    "- Don't need an `add` function because we add it all at once instead of one-by-one.\n",
    "- Don't need multiple environments\n",
    "\n",
    "And for `ReplayMinibatch`\n",
    "- Don't need `actions` anymore since there isn't a sense of an \"agent\" since actions (tokens generated) are contained within the sequences\n",
    "- Don't need `dones` since we set the sequence to be `gen_len` long\n",
    "- Sotre `ref_logits` as a part of the KL penalty w.r.t the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplayMinibatch:\n",
    "    sample_ids: Float[Tensor, \"minibatch_size seq_len\"]\n",
    "    logprobs: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    advantages: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    returns: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"]\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, args, sample_ids, logprobs, advantages, values, ref_logits):\n",
    "        self.args = args\n",
    "        self.sample_ids = sample_ids\n",
    "        self.logprobs = logprobs\n",
    "        self.advantages = advantages\n",
    "        self.values = values\n",
    "        self.ref_logits = ref_logits\n",
    "\n",
    "    def get_minibatches(self):\n",
    "        minibatches = []\n",
    "\n",
    "        # since we use 1-step advantage estimation\n",
    "        # returns = next-step estimate of value function\n",
    "        returns = self.advantages + self.values[:, -self.args.gen_len - 1: -1]\n",
    "\n",
    "        for _ in range(self.args.batches_per_learning_phase):\n",
    "            for indices in t.randperm(self.args.batch_size).reshape(self.args.num_minibatches, -1):\n",
    "                minibatches.append(ReplayMinibatch(\n",
    "                    sample_ids = self.sample_ids[indices],\n",
    "                    logprobs=self.logprobs[indices],\n",
    "                    advantages=self.advantages[indices],\n",
    "                    returns=returns[indices],\n",
    "                    ref_logits=self.ref_logits[indices]\n",
    "                ))\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907caad2",
   "metadata": {},
   "source": [
    "In addition to the 3 components of the total PPO objective, we'll add on the KL penalty as a part of the RLHF framework.\n",
    "- The KL prediction shift penalty is $-\\lambda_{KL} D_{KL}(\\pi_{PPO}\\phantom{.}|| \\phantom{.}\\pi_{base})$ (and not the other way) because the penalization should be for results that are likely under $\\pi_{PPO}$ and unlikely under $\\pi_{base}$. Expanding the KL penalty yields: $$\\lambda_{KL} \\cdot \\sum_i \\pi_{PPO_i}\\log\\left(\\frac{\\pi_{PPO_i}}{\\pi_{base_i}}\\right)$$\n",
    "- The `entropy`, `value_fn`, and `clipped_sur_obj` functions are essentially the same from PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aeb43bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .mean() to aggregate over the batch + stabilize training\n",
    "def calc_kl_penalty(logits, ref_logits, kl_coef):\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    ref_log_probs = ref_logits.log_softmax(-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    kl_div = (probs * (log_probs - ref_log_probs)).sum(-1)\n",
    "\n",
    "    return kl_coef * kl_div.mean()\n",
    "\n",
    "def calc_entropy_bonus(logits, ent_coef):\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    entropy = -(log_probs * probs).sum(-1)\n",
    "\n",
    "    return ent_coef * entropy.mean()\n",
    "\n",
    "# supervised regression loss for the value function\n",
    "def calc_value_fn_loss(values, mb_returns, vf_coef):\n",
    "    return 1/2 * vf_coef * (values - mb_returns).pow(2).mean()\n",
    "\n",
    "def calc_clipped_sur_obj(logprobs, mb_logprobs, mb_advantages, clip_coef, eps = 1e-8):\n",
    "    logits_diff = logprobs - mb_logprobs\n",
    "    # ratio of the policies\n",
    "    ratio = t.exp(logits_diff)\n",
    "\n",
    "    # normalizing the advantages\n",
    "    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n",
    "\n",
    "    # standard clip application\n",
    "    non_clipped = ratio * mb_advantages\n",
    "    clipped = t.clip(ratio, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n",
    "\n",
    "    return t.minimum(non_clipped, clipped).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f41984",
   "metadata": {},
   "source": [
    "`get_log_probs` ensures that the output is always of size `(minibatch_size, gen_len)`. We only care about the log probs of the tokens generated, not in the prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7e760a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(logits, tokens, prefix_len):\n",
    "    if prefix_len is not None:\n",
    "        logits = logits[:, prefix_len-1:]\n",
    "        tokens = tokens[:, prefix_len-1:]\n",
    "    \n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    shaped_log_probs = eindex(log_probs, tokens, \"b s [b s+1]\")\n",
    "\n",
    "    return shaped_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2e8f1",
   "metadata": {},
   "source": [
    "For both the base model and the value head, we define seperate learning rates, which makes sense since the value head is randomly initalized whereas the base model is already built out.\n",
    "\n",
    "For the scheduler, we use a lienar warmup up to `1.0` then linear decay down to `args.final_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94c881ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, base_lr, head_lr):\n",
    "    return t.optim.AdamW(\n",
    "        [\n",
    "           {\"params\": model.base_model.parameters(), \"lr\": base_lr},\n",
    "           {\"params\": model.value_head.parameters(), \"lr\": head_lr} \n",
    "        ], maximize = True)\n",
    "\n",
    "def get_optimizer_and_scheduler(args, model):\n",
    "    def lr_lambda(step):\n",
    "        if step < args.warmup_steps:\n",
    "            return step / args.warmup_steps\n",
    "        else:\n",
    "            return 1 - (1 - args.final_scale) * (step - args.warmup_steps) / (args.total_phases - args.warmup_steps)\n",
    "        \n",
    "    optimizer = get_optimizer(model, args.base_lr, args.head_lr)\n",
    "    scheduler = t.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lr_lambda)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffef61",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f53d9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLHFTrainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.run_name = f\"{args.wandb_project_name}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "        self.model = TransformerWithValueHead(args.base_model).to(device).train()\n",
    "        self.ref_model = HookedTransformer.from_pretrained(args.base_model).to(device).eval()\n",
    "        self.optimizer, self.scheduler = get_optimizer_and_scheduler(self.args, self.model)\n",
    "        self.prefix_len = len(self.model.base_model.to_str_tokens(self.args.prefix, prepend_bos = self.args.prepend_bos))\n",
    "\n",
    "    def compute_rlhf_objective(self, minibatch):\n",
    "        logits, values = self.model(minibatch.sample_ids)\n",
    "        log_probs = get_log_probs(logits, minibatch.sample_ids, self.prefix_len)\n",
    "\n",
    "        gen_len_slice = slice(-self.args.gen_len - 1, -1)\n",
    "\n",
    "        # ??? understand the slicing and why it differs for minibatch and log_probs\n",
    "        kl_penalty = calc_kl_penalty(logits[:, gen_len_slice], minibatch.ref_logits[:, gen_len_slice], self.args.kl_coef)\n",
    "        entropy = calc_entropy_bonus(logits[:, gen_len_slice], self.args.ent_coef)\n",
    "        value_fn_loss = calc_value_fn_loss(values[:, gen_len_slice], minibatch.returns, self.args.vf_coef)\n",
    "        clipped_sur_obj = calc_clipped_sur_obj(log_probs, minibatch.logprobs, minibatch.advantages, self.args.clip_coef)\n",
    "\n",
    "        ppo_obj_fn = clipped_sur_obj - value_fn_loss + entropy\n",
    "        total_obj_fn = ppo_obj_fn - kl_penalty\n",
    "\n",
    "        # RL-specific logging\n",
    "        with t.inference_mode():\n",
    "            logratio = log_probs - minibatch.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n",
    "        wandb.log(\n",
    "            dict(\n",
    "                total_steps=self.step,\n",
    "                lr=self.scheduler.get_last_lr()[0],\n",
    "                clipped_surrogate_objective=clipped_sur_obj.item(),\n",
    "                clipfrac=np.mean(clipfracs),\n",
    "                value_loss=value_fn_loss.item(),\n",
    "                values=values.mean().item(),\n",
    "                entropy_bonus=entropy.item(),\n",
    "                kl_penalty=kl_penalty.item(),\n",
    "            ),\n",
    "            step=self.step,\n",
    "        )\n",
    "\n",
    "        return total_obj_fn\n",
    "    \n",
    "    def rollout_phase(self):\n",
    "        sample_ids, samples = get_samples(\n",
    "            base_model = self.model.base_model,\n",
    "            prompt = self.args.prefix,\n",
    "            batch_size = self.args.batch_size,\n",
    "            gen_len = self.args.gen_len,\n",
    "            temperature = self.args.temperature,\n",
    "            top_k = self.args.top_k,\n",
    "            prepend_bos = self.args.prepend_bos)\n",
    "        \n",
    "        with t.inference_mode():\n",
    "            logits, values = self.model(sample_ids)\n",
    "            ref_logits = self.ref_model(sample_ids)\n",
    "\n",
    "        log_probs = get_log_probs(logits, sample_ids, self.prefix_len)\n",
    "\n",
    "        rewards = self.args.reward_fn(samples)\n",
    "        rewards_mean = rewards.mean().item()\n",
    "        rewards_normed = normalize_reward(rewards) if self.args.normalize_reward else rewards\n",
    "        advantages = compute_advantages(values, rewards_normed, self.prefix_len)\n",
    "\n",
    "        wandb.log({\"Mean Reward\": rewards_mean}, step = self.step)\n",
    "\n",
    "        # visualization\n",
    "        n_log_samples = min(3, self.args.batch_size)\n",
    "        ref_logprobs = get_log_probs(ref_logits[:n_log_samples], sample_ids[:n_log_samples], self.prefix_len).sum(-1)\n",
    "        headers = [\"Reward\", \"Ref logprobs\", \"Sample\"]\n",
    "        table_data = [[str(int(r)), f\"{lp:.2f}\", repr(s)] for r, lp, s in zip(rewards.tolist(), ref_logprobs, samples)]\n",
    "        table = tabulate(table_data, headers, tablefmt=\"simple_grid\", maxcolwidths=[None, None, 90])\n",
    "        print(f\"Phase {self.phase+1:03}/{self.args.total_phases:03}, Mean reward: {rewards_mean:.4f}\\n{table}\\n\")\n",
    "\n",
    "        return ReplayMemory(\n",
    "            args = self.args,\n",
    "            sample_ids = sample_ids,\n",
    "            logprobs = log_probs,\n",
    "            advantages = advantages,\n",
    "            values = values,\n",
    "            ref_logits = ref_logits)\n",
    "        \n",
    "    def learning_phase(self, memory):\n",
    "        for minibatch in tqdm(memory.get_minibatches(), desc=f"Learning phase {self.phase+1}", leave=False):\n",
    "            self.optimizer.zero_grad()\n",
    "            total_obj_fn = self.compute_rlhf_objective(minibatch)\n",
    "            total_obj_fn.backward()\n",
    "            # clip according to max_norm\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = self.args.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.step += 1\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def train(self):\n",
    "        # ??? why define these here instead of global\n",
    "        self.step = 0\n",
    "        self.samples = []\n",
    "\n",
    "        wandb.init(\n",
    "            project = self.args.wandb_project_name,\n",
    "            entity = self.args.wandb_entity,\n",
    "            name = self.run_name,\n",
    "            config = self.args,
            reinit = True\n",
    "        )\n",
    "\n",
    "        for self.phase in tqdm(range(self.args.total_phases), desc="Training phases"):\n",
    "            memory = self.rollout_phase()\n",
    "            self.learning_phase(memory)\n",
    "\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "176ea683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rlhf_transformers_20250718-213412</strong> at: <a href='https://wandb.ai/djdumpling-yale/rlhf_transformers/runs/1scyfotl' target=\"_blank\">https://wandb.ai/djdumpling-yale/rlhf_transformers/runs/1scyfotl</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/rlhf_transformers' target=\"_blank\">https://wandb.ai/djdumpling-yale/rlhf_transformers</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250718_213420-1scyfotl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alexwa/Documents/GitHub/rl/rlhf_transformer/wandb/run-20250718_214014-fvvm6a5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djdumpling-yale/rlhf_transformers/runs/fvvm6a5v' target=\"_blank\">rlhf_transformers_20250718-214003</a></strong> to <a href='https://wandb.ai/djdumpling-yale/rlhf_transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djdumpling-yale/rlhf_transformers' target=\"_blank\">https://wandb.ai/djdumpling-yale/rlhf_transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djdumpling-yale/rlhf_transformers/runs/fvvm6a5v' target=\"_blank\">https://wandb.ai/djdumpling-yale/rlhf_transformers/runs/fvvm6a5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 001/005, Mean reward: 0.8862\n",
      "┌──────────┬────────────────┬────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│   Reward │   Ref logprobs │ Sample                                                                                     │\n",
      "├──────────┼────────────────┼────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│        0 │         -96.29 │ \"<|endoftext|>This movie was really fun. I've been looking forward to watching this movie  │\n",
      "│          │                │ for a while now because I have so much to say. It's really hard to watch a movie like      │\n",
      "│          │                │ this, because you've seen so many amazing people make films that you just can't seem\"      │\n",
      "├──────────┼────────────────┼────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│        0 │        -119.3  │ '<|endoftext|>This movie was really good and it has great music as well. It was also       │\n",
      "│          │                │ amazing to see a lot of action in the background which you might think would be difficult  │\n",
      "│          │                │ to achieve. It also had a lot of action, and I would not recommend watching this film      │\n",
      "│          │                │ without some'                                                                              │\n",
      "├──────────┼────────────────┼────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│        0 │        -105.82 │ \"<|endoftext|>This movie was really fun to watch. The action was pretty well paced as well │\n",
      "│          │                │ as well paced, with lots of different scenes and characters that were just fun to watch.   │\n",
      "│          │                │ It is a bit of a mystery to watch, but it's fun and a fun movie as well\"                   │\n",
      "└──────────┴────────────────┴────────────────────────────────────────────────────────────────────────────────────────────┘\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReplayMinibatch() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m args = RLHFArgs(kl_coef=\u001b[32m0.0\u001b[39m, total_phases=\u001b[32m5\u001b[39m, warmup_steps=\u001b[32m0\u001b[39m, reward_fn=reward_fn_sentiment_imdb)\n\u001b[32m      3\u001b[39m trainer = RLHFTrainer(args)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mRLHFTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.phase \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.args.total_phases):\n\u001b[32m    111\u001b[39m     memory = \u001b[38;5;28mself\u001b[39m.rollout_phase()\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlearning_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    114\u001b[39m wandb.finish()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mRLHFTrainer.learning_phase\u001b[39m\u001b[34m(self, memory)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearning_phase\u001b[39m(\u001b[38;5;28mself\u001b[39m, memory):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m minibatch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmemory\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_minibatches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     88\u001b[39m         \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m     89\u001b[39m         total_obj_fn = \u001b[38;5;28mself\u001b[39m.compute_rlhf_objective(minibatch)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mReplayMemory.get_minibatches\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.args.batches_per_learning_phase):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m t.randperm(\u001b[38;5;28mself\u001b[39m.args.batch_size).reshape(\u001b[38;5;28mself\u001b[39m.args.num_minibatches, -\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         minibatches.append(\u001b[43mReplayMinibatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43msample_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msample_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43mref_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mref_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m minibatches\n",
      "\u001b[31mTypeError\u001b[39m: ReplayMinibatch() takes no arguments"
     ]
    }
   ],
   "source": [
    "# testing with kl_coef = 0.0, it has no incentives to match the ref distribution, only maximize reward\n",
    "args = RLHFArgs(kl_coef=0.0, total_phases=5, warmup_steps=0, reward_fn=reward_fn_sentiment_imdb)\n",
    "trainer = RLHFTrainer(args)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
