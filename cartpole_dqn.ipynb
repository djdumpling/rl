{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/cartpole_dqn_overview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import wandb\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn, Tensor\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Arr: np.ndarray\n",
    "\n",
    "device = t.device(\"mps\") if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else t.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only two actions to take: move the cart left or right. We observe 4 numbers: position, velocity, angle, and the angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the QNetwork with a simple 3-layer NN with 10k parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 10934\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, obs_shape, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_shape[0], 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "net = QNetwork(obs_shape = (4,), num_actions = 2)\n",
    "\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the replay buffer. Thought the add function was neat since it slices off old elements. \n",
    "Since using mps, need to tensorify the 5 returned arrays.\n",
    "\n",
    "I'm curious much the capacity affects the rate of catastrophic forgetting. Maybe exponential decay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, obs_shape, action_shape, capacity, seed):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.capacity = int(capacity)\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # obs, actions, rewards, next_obs, terminated\n",
    "        self.obs = np.empty((0, *self.obs_shape), dtype = np.float32)\n",
    "        self.actions = np.empty((0, *self.action_shape), dtype = np.int32)\n",
    "        self.rewards = np.empty(0, dtype = np.float32)\n",
    "        self.next_obs = np.empty((0, *self.obs_shape), dtype = np.float32)\n",
    "        self.terminated = np.empty(0, dtype = bool)\n",
    "\n",
    "    def add(self, obs, actions, rewards, next_obs, terminated):\n",
    "        # For single environment, we need to add batch dimension to match buffer structure\n",
    "        \n",
    "        # obs: shape (obs_features,) -> (1, obs_features)\n",
    "        if obs.ndim == 1:\n",
    "            obs = obs[np.newaxis, :]\n",
    "        \n",
    "        # actions: scalar or shape () -> (1,)\n",
    "        if np.isscalar(actions):\n",
    "            actions = np.array([actions])\n",
    "        elif actions.ndim == 0:\n",
    "            actions = actions[np.newaxis]\n",
    "        \n",
    "        # rewards: scalar or shape () -> (1,)\n",
    "        if np.isscalar(rewards):\n",
    "            rewards = np.array([rewards])\n",
    "        elif rewards.ndim == 0:\n",
    "            rewards = rewards[np.newaxis]\n",
    "        \n",
    "        # next_obs: shape (obs_features,) -> (1, obs_features)\n",
    "        if next_obs.ndim == 1:\n",
    "            next_obs = next_obs[np.newaxis, :]\n",
    "        \n",
    "        # terminated: scalar or shape () -> (1,)\n",
    "        if np.isscalar(terminated):\n",
    "            terminated = np.array([terminated])\n",
    "        elif terminated.ndim == 0:\n",
    "            terminated = terminated[np.newaxis]\n",
    "\n",
    "        # Now concatenate with proper shapes\n",
    "        self.obs = np.concatenate((self.obs, obs))[-self.capacity:]\n",
    "        self.actions = np.concatenate((self.actions, actions))[-self.capacity:]\n",
    "        self.rewards = np.concatenate((self.rewards, rewards))[-self.capacity:]\n",
    "        self.next_obs = np.concatenate((self.next_obs, next_obs))[-self.capacity:]\n",
    "        self.terminated = np.concatenate((self.terminated, terminated))[-self.capacity:]\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        # Sample from current buffer size, not capacity\n",
    "        current_size = len(self.obs)\n",
    "        if current_size == 0:\n",
    "            raise ValueError(\"Cannot sample from empty buffer\")\n",
    "        \n",
    "        indices = self.rng.integers(0, current_size, size = batch_size)\n",
    "\n",
    "        obs_tensor = t.tensor(self.obs[indices], dtype = t.float32, device = device)\n",
    "        actions_tensor = t.tensor(self.actions[indices], dtype = t.long, device = device)  # Use long for actions\n",
    "        rewards_tensor = t.tensor(self.rewards[indices], dtype = t.float32, device = device)\n",
    "        next_obs_tensor = t.tensor(self.next_obs[indices], dtype = t.float32, device = device)\n",
    "        terminated_tensor = t.tensor(self.terminated[indices], dtype = t.bool, device = device)\n",
    "\n",
    "        return obs_tensor, actions_tensor, rewards_tensor, next_obs_tensor, terminated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only made sense to me mathetically until I realized it's analagous to ReLU being max(0,x)\n",
    "\n",
    "Still don't exactly understand what's going on behind the scenes with `.detach().cpu().numpy()`, will need to dig a little deeper into the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(curr_step, start_e, end_e, exploration_fraction, total_timesteps):\n",
    "    return start_e + (end_e - start_e) * min(curr_step / (exploration_fraction * total_timesteps), 1)\n",
    "\n",
    "# returns the sampled action for each env\n",
    "def epsilon_greedy_policy(envs, q_net, obs, epsilon):\n",
    "    obs = t.from_numpy(obs).float().to(device)  # Move tensor to correct device\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(0, envs.action_space.n)\n",
    "    else:\n",
    "        q_values = q_net(obs)\n",
    "        return q_values.argmax().detach().cpu().numpy().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining standard arguments for a DQN, for global, wandb, durations, hyperparameters, and rl-specific stuff.\n",
    "Learned that `@dataclass` is for specific for classes that holds memory, automatically initializes stuff like `def __init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DQNArgs:\n",
    "    \n",
    "    seed = 0\n",
    "    env_id = \"CartPole\"\n",
    "\n",
    "    wandb_project_name = 'DQN CartPole'\n",
    "    wandb_entity = None\n",
    "    video_log_freq = 10000\n",
    "\n",
    "    total_timesteps = 5e5\n",
    "    steps_per_train = 1e1\n",
    "    trains_per_target_update = 1e2\n",
    "    buffer_size = 1e4\n",
    "\n",
    "    batch_size = 128\n",
    "    learning_rate = 2.5e-4\n",
    "\n",
    "    gamma = 0.99\n",
    "    start_e = 1.0\n",
    "    end_e = 0.1\n",
    "    exploration_fraction = 0.2\n",
    "\n",
    "    def __post_init__(self):\n",
    "        import pathlib\n",
    "        self.total_training_steps = int((self.total_timesteps - self.buffer_size) // self.steps_per_train)\n",
    "        \n",
    "        # Create video save directory\n",
    "        section_dir = pathlib.Path.cwd()\n",
    "        self.video_save_path = section_dir / \"videos\"\n",
    "        self.video_save_path.mkdir(exist_ok=True)\n",
    "\n",
    "args = DQNArgs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard implementation of our DQN Agent. We use true_next_obs, an augmented version of our next_obs with the information of whether we are terminated or truncated. Every single step, we add to the buffer and reset our observation, ready for the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, envs, buffer, q_network, start_e, end_e, exploration_fraction, total_timesteps):\n",
    "        self.envs = envs\n",
    "        self.buffer = buffer\n",
    "        self.q_network = q_network\n",
    "        self.start_e = start_e\n",
    "        self.end_e = end_e\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "        self.step = 0\n",
    "        self.obs, _ = envs.reset()\n",
    "        self.epsilon = start_e\n",
    "\n",
    "    def get_actions(self, obs):\n",
    "        self.epsilon = linear_schedule(self.step, self.start_e, self.end_e, self.exploration_fraction, self.total_timesteps)\n",
    "        actions = epsilon_greedy_policy(self.envs, self.q_network, self.obs, self.epsilon)\n",
    "        return actions\n",
    "\n",
    "    def play_step(self):\n",
    "        self.obs = np.array(self.obs, dtype = np.float32)\n",
    "        actions = self.get_actions(self.obs)\n",
    "        next_obs, reward, terminated, truncated, infos = self.envs.step(actions)\n",
    "\n",
    "        true_next_obs = next_obs.copy()\n",
    "        if terminated | truncated:\n",
    "            # Check if final_observation exists in infos, otherwise use next_obs\n",
    "            if \"final_observation\" in infos:\n",
    "                true_next_obs = infos[\"final_observation\"]\n",
    "            else:\n",
    "                true_next_obs = next_obs\n",
    "\n",
    "        self.buffer.add(self.obs, actions, reward, true_next_obs, terminated)\n",
    "        \n",
    "        # Reset environment if episode ended\n",
    "        if terminated | truncated:\n",
    "            self.obs, _ = self.envs.reset()\n",
    "        else:\n",
    "            self.obs = next_obs\n",
    "        \n",
    "        self.step += 1\n",
    "\n",
    "        return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives a dict of the episode length & reward & duration for the first terminated env, or `None` if no envs terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_data_from_infos(infos):\n",
    "    for final_info in infos.get(\"final_info\", []):\n",
    "        if final_info is not None and \"episode\" in final_info:\n",
    "            return {\"episode_length\": final_info[\"episode\"][\"l\"].item(), \n",
    "                    \"episode_reward\": final_info[\"episode\"][\"r\"].item(),\n",
    "                    \"episode_duration\": final_info[\"episode\"][\"t\"].item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.rng = np.random.default_rng(args.seed)\n",
    "        self.run_name = f\"{args.env_id}_{args.wandb_project_name}__seed{args.seed}__{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        self.envs = env\n",
    "\n",
    "        # Create video recording environment\n",
    "        if args.video_log_freq is not None:\n",
    "            self.video_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "            # Wrap with RecordVideo for automatic video recording\n",
    "            from gymnasium.wrappers import RecordVideo\n",
    "            self.video_env = RecordVideo(\n",
    "                self.video_env,\n",
    "                video_folder=str(args.video_save_path),\n",
    "                episode_trigger=lambda x: True,  # Record every episode during evaluation\n",
    "                name_prefix=\"dqn_cartpole\"\n",
    "            )\n",
    "        else:\n",
    "            self.video_env = None\n",
    "\n",
    "        action_shape = self.envs.action_space.shape\n",
    "        num_actions = self.envs.action_space.n\n",
    "        obs_shape = self.envs.observation_space.shape\n",
    "\n",
    "        self.buffer = ReplayBuffer(obs_shape, action_shape, args.buffer_size, args.seed)\n",
    "\n",
    "        self.q_network = QNetwork(obs_shape, num_actions).to(device)\n",
    "        self.target_network = QNetwork(obs_shape, num_actions).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = t.optim.AdamW(self.q_network.parameters(), lr = args.learning_rate)\n",
    "\n",
    "        self.agent = DQNAgent(self.envs,self.buffer, self.q_network, args.start_e, args.end_e, args.exploration_fraction, args.total_timesteps)\n",
    "\n",
    "    def add_to_replay_buffer(self, n: int, verbose = False):\n",
    "        data = None\n",
    "        t0 = time.time()\n",
    "\n",
    "        for step in tqdm(range(n), disable = not verbose):\n",
    "            infos = self.agent.play_step()\n",
    "            new_data = get_episode_data_from_infos(infos)\n",
    "\n",
    "            if new_data is not None:\n",
    "                data = new_data\n",
    "                wandb.log(new_data, step = self.agent.step)\n",
    "\n",
    "        wandb.log({\"Samples per second\": n / (time.time() - t0)}, step = self.agent.step)\n",
    "        return data\n",
    "    \n",
    "    def prepopulate_replay_buffer(self):\n",
    "        n_steps_to_fill_buffer = self.args.buffer_size\n",
    "        self.add_to_replay_buffer(int(n_steps_to_fill_buffer))\n",
    "    \n",
    "    def record_video_episode(self):\n",
    "        \"\"\"Record a single episode for video logging\"\"\"\n",
    "        if self.video_env is None:\n",
    "            return None\n",
    "            \n",
    "        obs, _ = self.video_env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not (terminated or truncated):\n",
    "            obs_tensor = t.from_numpy(obs).float().to(device)\n",
    "            with t.no_grad():\n",
    "                q_values = self.q_network(obs_tensor)\n",
    "                action = q_values.argmax().item()  # Use greedy policy for evaluation\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = self.video_env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "        return {\"eval_episode_reward\": episode_reward, \"eval_episode_length\": episode_length}\n",
    "    \n",
    "    def log_videos_to_wandb(self):\n",
    "        \"\"\"Find and log the most recent video to wandb\"\"\"\n",
    "        import glob\n",
    "        import os\n",
    "        \n",
    "        if self.video_env is None:\n",
    "            return\n",
    "            \n",
    "        # Find the most recent video file\n",
    "        video_files = glob.glob(str(self.args.video_save_path / \"*.mp4\"))\n",
    "        if video_files:\n",
    "            # Get the most recent video file\n",
    "            latest_video = max(video_files, key=os.path.getctime)\n",
    "            \n",
    "            # Log video to wandb\n",
    "            wandb.log({\n",
    "                \"eval_video\": wandb.Video(latest_video, fps=30, format=\"mp4\")\n",
    "            }, step=self.agent.step)\n",
    "    \n",
    "    def training_step(self, step):\n",
    "        obs, actions, rewards, next_obs, terminated = self.buffer.sample(self.args.batch_size, device)\n",
    "\n",
    "        with t.inference_mode():\n",
    "            target_max = self.target_network(next_obs).max(-1).values\n",
    "        predicted_q_vals = self.q_network(obs)[range(len(actions)), actions]\n",
    "\n",
    "        td_error = rewards + (1 - terminated.float()) * self.args.gamma * target_max - predicted_q_vals\n",
    "        loss = td_error.pow(2).mean()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        if step % self.args.trains_per_target_update == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "\n",
    "        wandb.log({\"td_loss\": loss, \"q_values\": predicted_q_vals.mean().item(), \"epsilon\": self.agent.epsilon}, step = self.agent.step)\n",
    "\n",
    "    def train(self):\n",
    "        wandb.init(project = self.args.wandb_project_name, \n",
    "                   entity = self.args.wandb_entity,\n",
    "                   name = self.run_name,\n",
    "                   monitor_gym = False)  # We'll handle video logging manually\n",
    "        wandb.watch(self.q_network, log = \"all\", log_freq = 50)\n",
    "\n",
    "        pbar = tqdm(range(self.args.total_training_steps))\n",
    "        last_logged_time = time.time()\n",
    "\n",
    "        for step in pbar:\n",
    "            data = self.add_to_replay_buffer(int(self.args.steps_per_train))\n",
    "            if data is not None and time.time() - last_logged_time > 0.50:\n",
    "                last_logged_time = time.time()\n",
    "                pbar.set_postfix(**data)\n",
    "\n",
    "            self.training_step(step)\n",
    "            \n",
    "            # Record and log videos at specified frequency\n",
    "            if (self.args.video_log_freq is not None and \n",
    "                step > 0 and \n",
    "                step % self.args.video_log_freq == 0):\n",
    "                \n",
    "                eval_data = self.record_video_episode()\n",
    "                if eval_data is not None:\n",
    "                    wandb.log(eval_data, step=self.agent.step)\n",
    "                    pbar.set_description(f\"Eval reward: {eval_data['eval_episode_reward']:.1f}\")\n",
    "                \n",
    "                self.log_videos_to_wandb()\n",
    "\n",
    "        # Record final video\n",
    "        if self.args.video_log_freq is not None:\n",
    "            eval_data = self.record_video_episode()\n",
    "            if eval_data is not None:\n",
    "                wandb.log(eval_data, step=self.agent.step)\n",
    "            self.log_videos_to_wandb()\n",
    "\n",
    "        self.envs.close()\n",
    "        if self.video_env is not None:\n",
    "            self.video_env.close()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 9999/49000 [01:46<08:24, 77.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-0.mp4.\n",
      "MoviePy - Writing video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 299.0:  20%|██        | 9999/49000 [01:46<08:24, 77.26it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "Eval reward: 299.0:  20%|██        | 10007/49000 [01:47<36:01, 18.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 299.0:  41%|████      | 20000/49000 [03:50<04:54, 98.38it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-1.mp4.\n",
      "MoviePy - Writing video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 380.0:  41%|████      | 20000/49000 [03:50<04:54, 98.38it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "Eval reward: 380.0:  41%|████      | 20010/49000 [03:50<19:49, 24.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 380.0:  61%|██████    | 29999/49000 [05:34<03:08, 100.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-2.mp4.\n",
      "MoviePy - Writing video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 500.0:  61%|██████    | 29999/49000 [05:35<03:08, 100.64it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "Eval reward: 500.0:  61%|██████    | 30010/49000 [05:35<13:08, 24.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 500.0:  82%|████████▏ | 39991/49000 [07:25<01:31, 98.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-3.mp4.\n",
      "MoviePy - Writing video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 500.0:  82%|████████▏ | 39991/49000 [07:26<01:31, 98.92it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n",
      "Eval reward: 500.0:  82%|████████▏ | 40010/49000 [07:26<05:54, 25.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-3.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval reward: 500.0: 100%|██████████| 49000/49000 [09:02<00:00, 90.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-4.mp4.\n",
      "MoviePy - Writing video /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /Users/alexwa/Documents/GitHub/rl/videos/dqn_cartpole-episode-4.mp4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Samples per second</td><td>█▆▃▄▄▂▂▂▅▃▂▁▂▃▄▁▂▃▃▃▃▂▃▃▃▃▃▃▃▃▁▃▄▃▃▂▂▃▃▃</td></tr><tr><td>epsilon</td><td>█▇▇▄▃▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>q_values</td><td>▁▁▃▄▄▅▅▅▆▆▆▆▆▆▆▇▆▆▆▆▆▇▇▇▇▇▇█████▇███████</td></tr><tr><td>td_loss</td><td>▁▁█▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Samples per second</td><td>1302.61934</td></tr><tr><td>epsilon</td><td>0.1</td></tr><tr><td>q_values</td><td>100.07554</td></tr><tr><td>td_loss</td><td>0.00183</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CartPole_DQN CartPole__seed0__2025-06-20_23-21-53</strong> at: <a href='https://wandb.ai/djdumpling-yale/DQN%20CartPole/runs/mppmzayl' target=\"_blank\">https://wandb.ai/djdumpling-yale/DQN%20CartPole/runs/mppmzayl</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/DQN%20CartPole' target=\"_blank\">https://wandb.ai/djdumpling-yale/DQN%20CartPole</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 58 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250620_232153-mppmzayl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 490000 that is less than the current step 490001. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tried to log to step 490000 that is less than the current step 490001. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = DQNArgs()\n",
    "trainer = DQNTrainer(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ReplayBuffer fixes...\n",
      "Buffer size after adding one sample: 1\n",
      "Buffer obs shape: (1, 4)\n",
      "Buffer actions shape: (1,)\n",
      "Buffer size after adding 4 samples: 4\n",
      "Sample shapes - obs: torch.Size([2, 4]), actions: torch.Size([2]), rewards: torch.Size([2])\n",
      "Buffer test successful!\n",
      "ReplayBuffer fixes work correctly!\n"
     ]
    }
   ],
   "source": [
    "test_args = DQNArgs()\n",
    "test_args.buffer_size = 10\n",
    "test_args.batch_size = 2\n",
    "\n",
    "buffer = ReplayBuffer((4,), (), test_args.buffer_size, 0)\n",
    "\n",
    "obs = np.array([1.0, 2.0, 3.0, 4.0], dtype=np.float32)  # Single obs\n",
    "action = 1  # Single action\n",
    "reward = 1.0  # Single reward\n",
    "next_obs = np.array([1.1, 2.1, 3.1, 4.1], dtype=np.float32)  # Single next_obs\n",
    "terminated = False  # Single terminated\n",
    "\n",
    "buffer.add(obs, action, reward, next_obs, terminated)\n",
    "print(f\"Buffer size after adding one sample: {len(buffer.obs)}\")\n",
    "print(f\"Buffer obs shape: {buffer.obs.shape}\")\n",
    "print(f\"Buffer actions shape: {buffer.actions.shape}\")\n",
    "\n",
    "for i in range(3):\n",
    "    buffer.add(obs + i, action, reward + i, next_obs + i, terminated)\n",
    "\n",
    "print(f\"Buffer size after adding 4 samples: {len(buffer.obs)}\")\n",
    "\n",
    "if len(buffer.obs) >= test_args.batch_size:\n",
    "    sample_obs, sample_actions, sample_rewards, sample_next_obs, sample_terminated = buffer.sample(test_args.batch_size, device)\n",
    "    print(f\"Sample shapes - obs: {sample_obs.shape}, actions: {sample_actions.shape}, rewards: {sample_rewards.shape}\")\n",
    "    print(\"Buffer test successful!\")\n",
    "else:\n",
    "    print(\"Not enough samples in buffer to test sampling\")\n",
    "\n",
    "print(\"ReplayBuffer fixes work correctly!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
