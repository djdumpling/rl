{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "204a3697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torchrl\n",
    "import wandb\n",
    "import functools\n",
    "from functools import partial\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "from eindex import eindex\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "import circuitsvis as cv\n",
    "import transformers\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "from transformer_lens.hook_points import (HookedRootModule, HookPoint)  # Hooking utilities\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel, AutoModelForCausalLM\n",
    "from torchrl.envs.llm import ChatEnv\n",
    "from torchrl.modules.llm import TransformersWrapper\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = t.device(\"cuda\" if t.cuda.is_available() else \"mps\" if t.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d27e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdjdumpling\u001b[0m (\u001b[33mdjdumpling-yale\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alexwa/Documents/GitHub/rl/rlhf_transformer/wandb/run-20250731_153223-upcg72re</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djdumpling-yale/rl-rlhf_transformer/runs/upcg72re' target=\"_blank\">good-flower-15</a></strong> to <a href='https://wandb.ai/djdumpling-yale/rl-rlhf_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djdumpling-yale/rl-rlhf_transformer' target=\"_blank\">https://wandb.ai/djdumpling-yale/rl-rlhf_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djdumpling-yale/rl-rlhf_transformer/runs/upcg72re' target=\"_blank\">https://wandb.ai/djdumpling-yale/rl-rlhf_transformer/runs/upcg72re</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-gpt2_20250730-004324:v0, 643.08MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:3.5 (183.3MB/s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef171c74d7547d0a1803b1344dcffe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358a2bf3285e4f4eb59a1e1069f8b680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad46410b2ff407e98fa84d63359a7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load artifact from wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('djdumpling-yale/rlhf_transformers/model-gpt2_20250730-004324:v0', type='model')\n",
    "model_dir = artifact.download()\n",
    "\n",
    "# set location locally \n",
    "model_path = os.path.join(model_dir, \"final_model.pt\")\n",
    "state_dict = t.load(model_path, map_location=device)\n",
    "\n",
    "# get weights from RLHFed model\n",
    "base_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith('base_model.'):\n",
    "        new_key = k.replace('base_model.', '')\n",
    "        base_state_dict[new_key] = v\n",
    "\n",
    "# load into gpt architecture\n",
    "source_model = AutoModelForCausalLM.from_pretrained(\"lvwerra/gpt2-imdb\")\n",
    "base_model = HookedTransformer.from_pretrained(\"gpt2\", device=device, hf_model = source_model)\n",
    "base_model.eval()\n",
    "\n",
    "rlhf_model = HookedTransformer.from_pretrained(\"gpt2\", device=device)\n",
    "rlhf_model.load_state_dict(base_state_dict)\n",
    "rlhf_model.eval()\n",
    "\n",
    "# if TRL comes in later?\n",
    "model = TransformersWrapper(model=rlhf_model, tokenizer=rlhf_model.tokenizer,input_mode=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91122dc9",
   "metadata": {},
   "source": [
    "Interestingly, it seems like the `base_model` already has a preference towards positive movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3cb996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473f123b842a42dba7671170ee71df8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was really really very good, and very well done. The music is great and the motion is great, and they are very friendly. It was so addicting and so very exciting. This is one of the best movies I've seen in my life, and\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a0f18f6d14c438a00c4557c9d2164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie was really bad. I don't think there were many redeeming qualities in watching this movie. The main character is really annoying; he is not good or interesting. He is annoying because he is a jerk who is going to tell you his life story, not\n"
     ]
    }
   ],
   "source": [
    "prompt = \"This movie was really\"\n",
    "\n",
    "print(rlhf_model.generate(prompt, max_new_tokens=50, temperature=0.7))\n",
    "print(base_model.generate(prompt, max_new_tokens=50, temperature=0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f671ad",
   "metadata": {},
   "source": [
    "This is reinforced when looking at the logits, showing that even the `base_model` has a preference towards positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd37b876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'This', ' movie', ' was', ' really']\n",
      "Tokenized answer: [' good']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.09</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.58</span><span style=\"font-weight: bold\">% Token: | good|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m15.09\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m5.58\u001b[0m\u001b[1m% Token: | good|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.45 Prob:  8.04% Token: | great|\n",
      "Top 1th token. Logit: 15.13 Prob:  5.85% Token: | amazing|\n",
      "Top 2th token. Logit: 15.09 Prob:  5.58% Token: | good|\n",
      "Top 3th token. Logit: 14.89 Prob:  4.57% Token: | fun|\n",
      "Top 4th token. Logit: 14.41 Prob:  2.83% Token: | awesome|\n",
      "Top 5th token. Logit: 14.26 Prob:  2.45% Token: | nice|\n",
      "Top 6th token. Logit: 13.88 Prob:  1.68% Token: | a|\n",
      "Top 7th token. Logit: 13.85 Prob:  1.62% Token: | really|\n",
      "Top 8th token. Logit: 13.72 Prob:  1.43% Token: | well|\n",
      "Top 9th token. Logit: 13.71 Prob:  1.41% Token: | hard|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' good'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' good'\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(prompt, \"good\", rlhf_model, prepend_bos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaefcd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'This', ' movie', ' was', ' really']\n",
      "Tokenized answer: [' good']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.39</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.19</span><span style=\"font-weight: bold\">% Token: | good|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m17.39\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m12.19\u001b[0m\u001b[1m% Token: | good|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 17.39 Prob: 12.19% Token: | good|\n",
      "Top 1th token. Logit: 17.30 Prob: 11.15% Token: | bad|\n",
      "Top 2th token. Logit: 16.20 Prob:  3.69% Token: | funny|\n",
      "Top 3th token. Logit: 16.18 Prob:  3.61% Token: | great|\n",
      "Top 4th token. Logit: 16.06 Prob:  3.21% Token: | a|\n",
      "Top 5th token. Logit: 15.95 Prob:  2.88% Token: | fun|\n",
      "Top 6th token. Logit: 15.87 Prob:  2.65% Token: | awful|\n",
      "Top 7th token. Logit: 15.57 Prob:  1.96% Token: | well|\n",
      "Top 8th token. Logit: 15.38 Prob:  1.63% Token: | terrible|\n",
      "Top 9th token. Logit: 15.34 Prob:  1.56% Token: | disappointing|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' good'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' good'\u001b[0m, \u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(prompt, \"good\", base_model, prepend_bos = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8648137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 922, 2089]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"This movie is really\"]\n",
    "answers = [(\" good\", \" bad\")]\n",
    "\n",
    "answer_tokens = []\n",
    "for i in range(len(prompts)):\n",
    "    answer_tokens.append((rlhf_model.to_single_token(answers[i][0]), rlhf_model.to_single_token(answers[i][1])))\n",
    "\n",
    "answer_tokens = t.tensor(answer_tokens).to(device)\n",
    "\n",
    "print(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39409b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = rlhf_model.to_tokens(prompts, prepend_bos = True)\n",
    "\n",
    "# base_logits.shape: [1,5,50257] --> # prompts, sequence length including prepend_bos , vocabulary size\n",
    "base_logits, base_cache = base_model.run_with_cache(tokens)\n",
    "rlhf_logits, rlhf_cache = rlhf_model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c66a25dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit diff for base model: -0.185\n",
      "Logit diff for rlhf model: 1.851\n"
     ]
    }
   ],
   "source": [
    "def logit_diff(logits, answer_tokens):\n",
    "    final_logit = logits[:, -1, :]\n",
    "    answer_logit = final_logit.gather(dim = -1, index = answer_tokens)\n",
    "    answer_logit_diff = answer_logit[:, 0] - answer_logit[:, 1]\n",
    "\n",
    "    return answer_logit_diff\n",
    "\n",
    "logit_diff_base = logit_diff(base_logits, answer_tokens).item()\n",
    "logit_diff_rlhf = logit_diff(rlhf_logits, answer_tokens).item()\n",
    "print(f\"Logit diff for base model: {logit_diff_base:.3f}\")\n",
    "print(f\"Logit diff for rlhf model: {logit_diff_rlhf:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff87a239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_residual_directions shape: [1, 2, 768] --> prompt, # tokens in answer_tokens, hidden size of model\n",
    "answer_residual_directions = rlhf_model.tokens_to_residual_directions(answer_tokens)\n",
    "logit_diff_directions = answer_residual_directions[:, 0, :] - answer_residual_directions[:, 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df3fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache syntax: [activation_name, layer_index, sub_layer_type]\n",
    "# final_residual_stream_xxx shape: [1, 5, 768]\n",
    "final_residual_stream_base = base_cache[\"resid_post\", -1]\n",
    "final_residual_stream_rlhf = rlhf_cache[\"resid_post\", -1]\n",
    "\n",
    "# final_token_residual_stream_base_xxxx shape: [1, 768]\n",
    "final_token_residual_stream_base = final_residual_stream_base[:, -1, :]\n",
    "final_token_residual_stream_rlhf = final_residual_stream_rlhf[:, -1, :]\n",
    "\n",
    "# layernorm scaling so that the contribution at each layer is consistent across the network\n",
    "scaled_final_token_residual_stream_base = base_cache.apply_ln_to_stack(final_token_residual_stream_base, layer = -1, pos_slice = -1)\n",
    "scaled_final_token_residual_stream_rlhf = rlhf_cache.apply_ln_to_stack(final_token_residual_stream_rlhf, layer = -1, pos_slice = -1)\n",
    "\n",
    "ln_logit_diff_base = einsum(\"batch d_model, batch d_model -> \", scaled_final_token_residual_stream_base, logit_diff_directions)\n",
    "ln_logit_diff_rlhf = einsum(\"batch d_model, batch d_model -> \", scaled_final_token_residual_stream_rlhf, logit_diff_directions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
