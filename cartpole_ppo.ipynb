{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Tumbling Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Not adding type annotations to my dataclass**. This makes the class-level default values rather than instance attributes that can be set via the constructor.\n",
    "2. **Mixing up standard deviations between actor & critic network**. The critic network needs a larger std (e.g. 1) to estimate returns over a widge range. The actor network needs a smaller std (e.g. 0.01) to make the policy more uniform at the beginning, which encourages exploration instead of action commitment. A small std for the actor network is *one of the most important initialisation details*.\n",
    "3. **Understanding we use np.empty((0, self.num_envs, ...))**. Also the shape doesn't make intuitive sense, this just essentially preps the experiences to concatenated on via `np.concatenate`. This avoids the awkward case of having the initalization step be different.\n",
    "4. **Understanding why we use .cpu().numpy()**. This is just for the Gym package, which expects data to be on the CPU and in the NumPy format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import warnings\n",
    "import time\n",
    "import einops\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from gymnasium.envs.classic_control import CartPoleEnv\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn, Tensor\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from tqdm import tqdm, trange\n",
    "from typing import Literal\n",
    "from helper_functions import set_global_seeds, make_env, get_episode_data_from_infos\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Arr: np.ndarray\n",
    "\n",
    "device = t.device(\"mps\") if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else t.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effectively @dataclass is always going to be paired with the Args class since it initalizes all of the arguments\n",
    "# and gets rid of the the annoying initalizations in def __init__(self, )\n",
    "@dataclass \n",
    "class PPOArgs:\n",
    "    seed: int = 1\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    mode: Literal[\"classic-control\", \"atari\", \"mujoco\"] = \"classic-control\"\n",
    "\n",
    "    total_timesteps: int = 500000\n",
    "    num_envs: int = 4\n",
    "    num_steps_per_rollout: int = 128\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 4\n",
    "\n",
    "    lr: float = 2.5e-4\n",
    "    max_grad_norm: float = 0.5\n",
    "\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.25\n",
    "\n",
    "    video_log_freq: int | None = None\n",
    "    project_name: str = \"Cartpole_PPO\"\n",
    "    entity: str = None\n",
    "\n",
    "    # comments in reference to num_minibatches = 2\n",
    "    def __post_init__(self):\n",
    "        self.batch_size = self.num_steps_per_rollout * self.num_envs # 512\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches # 256 \n",
    "        self.total_phases = self.total_timesteps // self.batch_size # 976\n",
    "        self.total_training_steps = self.total_phases * self.batches_per_learning_phase * self.num_minibatches # 7808\n",
    "\n",
    "        self.video_save_path = \"videos/cartpole_ppo\"\n",
    "        os.makedirs(self.video_save_path, exist_ok=True)\n",
    "\n",
    "args = PPOArgs(num_minibatches = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our actor and critic networks, which are just three layers with `nn.tanh()` in between. We initialize the layers in such a way that the signals' magnitudes are preserved, a flavor of He initialization. As mentioned before, the `std` of the last layers of the networks are very important.\n",
    "\n",
    "For now, we only define `get_actor_and_critic` in our CartPole case. The other modes will come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is essentially He initialization\n",
    "# orthogonality preserves magnitude and structures of signals as they propagate through the nn\n",
    "def layer_init(layer, std = np.sqrt(2), bias_const = 0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "def get_actor_and_critic_classic(num_obs, num_actions):\n",
    "    actor = nn.Sequential(\n",
    "        layer_init(nn.Linear(num_obs, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, num_actions), std = 0.01),\n",
    "    )\n",
    "\n",
    "    critic = nn.Sequential(\n",
    "        layer_init(nn.Linear(num_obs, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 64)),\n",
    "        nn.Tanh(),\n",
    "        layer_init(nn.Linear(64, 1), std = 1)\n",
    "    )\n",
    "\n",
    "    return actor, critic\n",
    "\n",
    "# returns the networks used for PPO\n",
    "# comments based on classic-control, atari, mujoco\n",
    "def get_actor_and_critic(envs, mode):\n",
    "    # (4, ), (84, 84, 4), (24, )\n",
    "    obs_shape = envs.single_observation_space.shape\n",
    "    # 4, 28224, 24\n",
    "    num_obs = np.array(obs_shape).prod()\n",
    "    # 2, 4, idk for mujuco yet\n",
    "    num_actions = (envs.single_action_space.n if isinstance(envs.single_action_space, gym.spaces.Discrete)\n",
    "                   else np.array(envs.single_action_space.shape).prod())\n",
    "    \n",
    "    if mode == \"classic-control\":  # Changed from classic_control to classic-control\n",
    "        actor, critic = get_actor_and_critic_classic(num_obs, num_actions)\n",
    "    if mode == \"atari\":\n",
    "        raise NotImplementedError()\n",
    "    if mode == \"mujoco\":\n",
    "        raise NotImplementedError()\n",
    "    return actor, critic  # Added missing return statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is GAE (Generalized Advantage Estimation)?**\n",
    "\n",
    "We care about the **advantage function** $A_\\theta(s_t, a_t)$, defined by $Q_\\theta(s_t, a_t) - V_\\theta(s_t)$ which represents the difference. between the expected future reward when taking action $a$ and the reward from taking the expected action according to policy $\\pi_\\theta$ from that point onwards.\n",
    "\n",
    "We start with the 1-step residual $\\hat{A}_\\theta(s_t, a_t) = \\delta_t = r_t + \\gamma \\cdot (1 - d_{t+1}) \\cdot V_\\theta(s_{t+1}) - V_\\theta(s_t)$, but this is too short-sighted. By combining the idea of summing future residuals and adding a geometrically decay factor to account for future residuals, we get $$\\hat{A}_t^{GAE(\\lambda)} = \\delta_t + (\\gamma \\lambda) \\cdot \\delta_{t+1} + \\cdots + (\\gamma \\lambda)^{T-t+1} \\delta_{T-1}$$\n",
    "\n",
    "We can get the recursive step $\\hat{A}_t^{GAE(\\lambda)} = \\delta_t + (1-d_{t+1}) \\cdot (\\gamma \\lambda) \\cdot \\hat{A}_{t+1}^{GAE(\\lambda)}$ which allows us to start from the final step and work forwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(next_value, next_terminated, rewards, values, terminated, gamma, gae_lambda):\n",
    "    # standardize as floats, also needed for the 1.0 - next_terminated\n",
    "    terminated = terminated.float()\n",
    "    next_terminated = next_terminated.float()\n",
    "\n",
    "    # Concatenate tensors along dim=0 (time dimension)\n",
    "    next_values = t.concat((values[1:], next_value[None, :]), dim=0)\n",
    "    next_terminated = t.concat((terminated[1:], next_terminated[None, :]), dim=0)\n",
    "\n",
    "    # calculated from above\n",
    "    deltas = rewards + gamma * (1.0 - next_terminated) * next_values - values\n",
    "    advantages = t.zeros_like(deltas)\n",
    "    \n",
    "    # update from the last element\n",
    "    advantages[-1] = deltas[-1]\n",
    "\n",
    "    # recursively iterate from the back\n",
    "    for s in reversed(range(values.shape[0] - 1)):\n",
    "        advantages[s] = deltas[s] + (1 - next_terminated[s]) * gamma * gae_lambda * advantages[s + 1]\n",
    "\n",
    "    return advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have `num_envs` vectorized environments and `num_steps_per_rollout`, then we have a total `batch_size = num_envs * num_steps_per_rollout`. Our goal is to randomly split those `batch_size` agent actions into minibatches with size `minibatch_size.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch_indices(rng, batch_size, minibatch_size):\n",
    "    num_minibatches = batch_size // minibatch_size\n",
    "\n",
    "    # numbers 0 -> batch_size-1, shaped by the num_minibatches\n",
    "    indices = rng.permutation(batch_size).reshape(num_minibatches, minibatch_size)\n",
    "    return list(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the `ReplayMinibatch` and the `ReplayMemory` which has all the necessary functions (`reset`, `add`, and `get_minibatches`). \n",
    "\n",
    "`reset` resets the memory; since PPO is on-policy, we use resets instead of slicing.\n",
    "\n",
    "`add` adds the most recent arrays related to the experience.\n",
    "\n",
    "`get_minibatches` get all the minibatches which are used as the batches to perform back propogation on the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate np for data storage and tensors for when torch is actually needed for grads, etc.\n",
    "# keeps all per-timestep rollout data neatly packaged, easier function interfaces\n",
    "@dataclass\n",
    "class ReplayMinibatch:\n",
    "    obs: Float[Tensor, \"minibatch_size *obs_shape\"]\n",
    "    actions: Int[Tensor, \"minibatch_size *action_shape\"]\n",
    "    logprobs: Float[Tensor, \"minibatch_size\"]\n",
    "    advantages: Float[Tensor, \"minibatch_size\"]\n",
    "    returns: Float[Tensor, \"minibatch_size\"]\n",
    "    terminated: Bool[Tensor, \"minibatch_size\"]\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, num_envs, obs_shape, action_shape, batch_size, minibatch_size, batches_per_learning_phase, seed: int = 42):\n",
    "        self.num_envs = num_envs\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.batches_per_learning_phase = batches_per_learning_phase\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.reset()\n",
    "\n",
    "    # *self.obs_shape and *self.action_shape allows for shape flexibility\n",
    "    def reset(self):\n",
    "        self.obs = np.empty((0, self.num_envs, *self.obs_shape), dtype=np.float32)\n",
    "        self.actions = np.empty((0, self.num_envs, *self.action_shape), dtype=np.int32)\n",
    "        self.logprobs = np.empty((0, self.num_envs), dtype=np.float32)\n",
    "        self.values = np.empty((0, self.num_envs), dtype=np.float32)\n",
    "        self.rewards = np.empty((0, self.num_envs), dtype=np.float32)\n",
    "        self.terminated = np.empty((0, self.num_envs), dtype=bool)\n",
    "\n",
    "    # using [None, :] adds another axis at the beginning, which is probably used as a time-step\n",
    "    def add(self, obs, actions, logprobs, values, rewards, terminated):\n",
    "        self.obs = np.concatenate((self.obs, obs[None, :]))\n",
    "        self.actions = np.concatenate((self.actions, actions[None, :]))\n",
    "        self.logprobs = np.concatenate((self.logprobs, logprobs[None, :]))\n",
    "        self.values = np.concatenate((self.values, values[None, :]))\n",
    "        self.rewards = np.concatenate((self.rewards, rewards[None, :]))\n",
    "        self.terminated = np.concatenate((self.terminated, terminated[None, :]))\n",
    "\n",
    "    def get_minibatches(self, next_value, next_terminated, gamma, gae_lambda):\n",
    "        obs, actions, logprobs, values, rewards, terminated = (\n",
    "            t.tensor(x, device = device, dtype = t.float32)\n",
    "            for x in [self.obs, self.actions, self.logprobs, self.values, self.rewards, self.terminated]\n",
    "        )\n",
    "\n",
    "        # these two will be useful in the critic learning value function, which depends on V_target, aka returns\n",
    "        advantages = compute_advantages(next_value, next_terminated, rewards, values, terminated, gamma, gae_lambda)\n",
    "        returns = advantages + values\n",
    "\n",
    "        minibatches = []\n",
    "        # we update on each experience more than once, particularly self.batches_per_learning_phase times\n",
    "        for _ in range(self.batches_per_learning_phase):\n",
    "            for indices in get_minibatch_indices(self.rng, self.batch_size, self.minibatch_size):\n",
    "                # not sure what the (0,1) means, but should combine the [T, num_envs, feature_dim] -> [T * num_envs, feature_dim]\n",
    "                minibatches.append(\n",
    "                    ReplayMinibatch(*[data.flatten(0, 1)[indices] \n",
    "                                    for data in [obs, actions, logprobs, advantages, returns, terminated]])\n",
    "                )\n",
    "\n",
    "        # clears the rollout buffer for the next training cycle\n",
    "        self.reset()\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the `PPOAgent` class, which includes two key functions.\n",
    "\n",
    "`play_step`: uses the actor network to get the policy, from which actions are sampled. Those actions are used in the environment which generates experiences stored in the replay memory. Then, `self.next_obs` and `self.next_terminated` are updated, as well as `self.step`.\n",
    "\n",
    "`get_minibatches`: basically uses `get_minibatches` from the `ReplayMinibatch` class after using the critic network to evaluate the advantage. It also resets the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent:\n",
    "    def __init__(self, envs, actor, critic, memory):\n",
    "        self.envs = envs\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.memory = memory\n",
    "\n",
    "        self.step = 0\n",
    "        self.next_obs = t.tensor(envs.reset()[0], device = device, dtype = t.float)\n",
    "        self.next_terminated = t.zeros(envs.num_envs, device = device, dtype = t.bool)\n",
    "\n",
    "    def play_step(self):\n",
    "        obs = self.next_obs\n",
    "        terminated = self.next_terminated\n",
    "\n",
    "        # f-prop to get the policy\n",
    "        # why no flatten here? maybe dist can't do it\n",
    "        with t.inference_mode():\n",
    "            logits = self.actor(obs)\n",
    "            values = self.critic(obs).flatten().cpu().numpy()\n",
    "\n",
    "        # sample from the policy to determine which action to take\n",
    "        dist = Categorical(logits = logits)\n",
    "        actions = dist.sample()\n",
    "\n",
    "        # take the action -> step\n",
    "        next_obs, rewards, next_terminated, next_truncated, infos = self.envs.step(actions.cpu().numpy())\n",
    "\n",
    "        # add everything into the memory\n",
    "        logprobs = dist.log_prob(actions).cpu().numpy()\n",
    "        self.memory.add(obs.cpu().numpy(), actions.cpu().numpy(), logprobs, values, rewards, terminated.cpu().numpy())\n",
    "\n",
    "        # update info for next step\n",
    "        self.next_obs = t.from_numpy(next_obs).to(device, dtype = t.float)\n",
    "        self.next_terminated = t.from_numpy(next_terminated).to(device, dtype = t.float)\n",
    "        self.step += self.envs.num_envs\n",
    "\n",
    "        return infos\n",
    "    \n",
    "    # not sure why we need .flatten()\n",
    "    def get_minibatches(self, gamma, gae_lambda):\n",
    "        # get the minibatches from the memory\n",
    "        with t.inference_mode():\n",
    "            next_value = self.critic(self.next_obs).flatten()\n",
    "        minibatches = self.memory.get_minibatches(next_value, self.next_terminated, gamma, gae_lambda)\n",
    "        self.memory.reset()\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we calculate the three components that will comrise the final training objective, given by $$L_t^{PPO}(\\theta) = \\hat{\\mathbb{E}}_t\\left[L_t^{CLIP}(\\theta) - c_1 L_t^{VF}(\\theta) + c_2 S[\\pi_\\theta](s_t)\\right]$$\n",
    "where the first reprents the PPO objective function $L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t\\left[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)\\right]$.\n",
    "\n",
    "The second represents the critic learning value function, defined by $$L^{VF}(\\theta) = (V_\\theta(s_t)-V_t^{target})^2$$ where $V_t^{target} = V_{\\theta_{target}}(s_t)+\\hat{A}_{\\theta_{target}}(s_t, a_t)$ is the return.\n",
    "\n",
    "The third represents the entropy which encourages exploration. The standard formula for a discrete probability distribution $p$ is given by $$H(p) = \\sum_x p(x) \\ln\\left(\\frac1{p(x)}\\right)$$ with the convention that $\\ln\\left(\\frac1{0}\\right)=0$. The larger the entropy, the more uncertain the action is, which essentially encourages exploration. If entropy is 0, there is no certainity and hence no exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_clipped_obj(dist, mb_action, mb_advantages, mb_logprobs, clip_coef, eps: float = 1e-8):\n",
    "    # we define prob_ratio as the ratio of the probabilities of new policy to the old policy\n",
    "    #  e^{logx - logy} = x/y, which is what we want\n",
    "    logits_diff = dist.log_prob(mb_action) - mb_logprobs\n",
    "    prob_ratio = t.exp(logits_diff)\n",
    "\n",
    "    # we standardize the mb_advantage\n",
    "    # standardization helps with training stability by keeping the advantages in a reasonable range\n",
    "    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n",
    "\n",
    "    # standard application of the formula\n",
    "    non_clipped = prob_ratio * mb_advantages\n",
    "    clipped = t.clip(prob_ratio, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n",
    "\n",
    "    return t.minimum(non_clipped, clipped).mean()\n",
    "\n",
    "# for MSE loss between predicted values and actual returns\n",
    "def calc_value_fn_loss(values, mb_returns, vf_coef):\n",
    "\n",
    "    return vf_coef * ((values - mb_returns)**2).mean()\n",
    "\n",
    "# to encourage exploration\n",
    "def calc_entropy(dist, ent_coef):\n",
    "    return ent_coef * dist.entropy().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a linear scheduler which decreases the learning rate from start (`initial_lr`) to end (`end_lr`). If we wanted it to decay for some period then stay constant afterwards, we would do something like in `cartpole_dqn.ipynb` and use the `min()` function, sorta like a ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOScheduler:\n",
    "    def __init__(self, optimizer, initial_lr, end_lr, total_phases):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.total_phases = total_phases\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    # the scheduler than linearly decays from initial_lr to end_lr\n",
    "    def step(self):\n",
    "        self.n_step_calls += 1\n",
    "        frac = self.n_step_calls / self.total_phases\n",
    "        # we update for both actor and critic\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = self.initial_lr + frac * (self.end_lr - self.initial_lr)\n",
    "\n",
    "# not sure: is it better to have end_lr = 0.0 at the end or still some small non-0 value?\n",
    "def make_optimizer(actor, critic, total_phases, initial_lr, end_lr = 0.0):\n",
    "    # passes all parameters (actor and critic) into a single optimizer\n",
    "    optimizer = optim.AdamW(itertools.chain(\n",
    "        actor.parameters(), critic.parameters()), lr = initial_lr, eps = 1e-5, maximize = True)\n",
    "    scheduler = PPOScheduler(optimizer, initial_lr, end_lr, total_phases)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOTrainer:\n",
    "    def __init__(self, args):\n",
    "        set_global_seeds(args.seed)\n",
    "        self.args = args\n",
    "        self.run_name = f\"{args.env_id}_{args.project_name}_seed{args.seed}__{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "        self.envs = gym.vector.SyncVectorEnv([\n",
    "            make_env(\n",
    "                env_id=args.env_id,\n",
    "                seed=args.seed + idx,\n",
    "                idx=idx,\n",
    "                run_name=self.run_name,\n",
    "                mode=args.mode,\n",
    "                video_log_freq=args.video_log_freq,\n",
    "                video_save_path=args.video_save_path\n",
    "            ) for idx in range(args.num_envs)\n",
    "        ])\n",
    "\n",
    "        self.num_envs = self.envs.num_envs\n",
    "        self.action_shape = self.envs.single_action_space.shape\n",
    "        self.obs_shape = self.envs.single_observation_space.shape\n",
    "\n",
    "        self.memory = ReplayMemory(self.num_envs, self.obs_shape, self.action_shape, args.batch_size, args.minibatch_size,\n",
    "                                   args.batches_per_learning_phase, args.seed)\n",
    "        \n",
    "        self.actor, self.critic = get_actor_and_critic(self.envs, mode = args.mode)\n",
    "        # Move networks to the correct device\n",
    "        self.actor = self.actor.to(device)\n",
    "        self.critic = self.critic.to(device)\n",
    "        \n",
    "        self.optimizer, self.scheduler = make_optimizer(self.actor, self.critic, args.total_training_steps, args.lr)\n",
    "\n",
    "        self.agent = PPOAgent(self.envs, self.actor, self.critic, self.memory)\n",
    "\n",
    "    # not sure what the shape of data is\n",
    "    # not sure why it is being overridden if there are multiple new_data s.t. new_data is not None\n",
    "    # in fact, i'm not sure how ts works\n",
    "    def rollout_phase(self):\n",
    "        data = None\n",
    "        t0 = time.time()\n",
    "        \n",
    "        for i in range(self.args.num_steps_per_rollout):\n",
    "            infos = self.agent.play_step()\n",
    "            new_data = get_episode_data_from_infos(infos)\n",
    "\n",
    "            # if the env terminated\n",
    "            if new_data is not None:\n",
    "                data = new_data\n",
    "                wandb.log(new_data, step = self.agent.step)\n",
    "\n",
    "        # samples per second, around 200\n",
    "        wandb.log({\"SPS\": (self.args.num_steps_per_rollout * self.num_envs) / (time.time() - t0)}, step = self.agent.step)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # forward direction, using the agent (actor/critic networks) to calculate the three functions and compute the total objective\n",
    "    def compute_ppo_objective(self, minibatch):\n",
    "        # actor policy, used in clipped_surrogate_obj\n",
    "        logits = self.actor(minibatch.obs)\n",
    "        dist = Categorical(logits = logits)\n",
    "\n",
    "        # not sure why we squeeze here\n",
    "        # values, used in the value fn loss\n",
    "        values = self.critic(minibatch.obs).squeeze()\n",
    "\n",
    "        clipped_obj = calc_clipped_obj(dist, minibatch.actions, minibatch.advantages, minibatch.logprobs, self.args.clip_coef)\n",
    "        value_fn_loss = calc_value_fn_loss(values, minibatch.returns, self.args.vf_coef)\n",
    "        entropy = calc_entropy(dist, self.args.ent_coef)\n",
    "        total_obj = clipped_obj - value_fn_loss + entropy\n",
    "\n",
    "        # purely for logging debug variables\n",
    "        with t.inference_mode():\n",
    "            new_log_prob = dist.log_prob(minibatch.actions)\n",
    "            log_ratio = new_log_prob - minibatch.logprobs\n",
    "            ratio = log_ratio.exp()\n",
    "            clip_frac = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n",
    "            approx_kl = ((ratio - 1) - log_ratio).mean().item()\n",
    "\n",
    "        # not sure why some of them have .mean() and why some of them have .item()\n",
    "        # for clip_frac is there a difference between np.mean(clip_frac) and clip_frac.mean()\n",
    "        wandb.log(dict(\n",
    "            total_steps = self.agent.step,\n",
    "            values = values.mean().item(),\n",
    "            lr = self.scheduler.optimizer.param_groups[0][\"lr\"],\n",
    "            clipped_obj = clipped_obj.item(),\n",
    "            value_fn_loss = value_fn_loss.item(),\n",
    "            entropy = entropy.item(),\n",
    "            approx_kl = approx_kl,\n",
    "            clip_frac = np.mean(clip_frac)\n",
    "        ), step = self.agent.step)\n",
    "\n",
    "        return total_obj\n",
    "    \n",
    "    # after calculating the total_obj, perform backprop to update weights\n",
    "    def learning_phase(self):\n",
    "        minibatches = self.agent.get_minibatches(self.args.gamma, self.args.gae_lambda)\n",
    "        for minibatch in minibatches:\n",
    "            total_obj = self.compute_ppo_objective(minibatch)\n",
    "            total_obj.backward()\n",
    "            # step recommended in the iclr blog\n",
    "            # global gradient clipping offers a small performance boost -> global l2 norm doesn't exceed 0.5\n",
    "            nn.utils.clip_grad_norm_(list(self.actor.parameters()) + list(self.critic.parameters()), self.args.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def train(self):\n",
    "        # not sure what entity and monitor_gym exactly do\n",
    "        wandb.init(project = self.args.project_name,\n",
    "                   entity = self.args.entity,\n",
    "                   name = self.run_name,\n",
    "                   monitor_gym = self.args.video_log_freq is not None)\n",
    "        # log parameter histograms, gradient norms, and updates\n",
    "        wandb.watch([self.actor, self.critic], log = \"all\", log_freq = 50)\n",
    "\n",
    "        pbar = tqdm(range(self.args.total_phases))\n",
    "        last_logged_time = time.time()\n",
    "\n",
    "        for phase in pbar:\n",
    "            data = self.rollout_phase()\n",
    "\n",
    "            if data is not None and time.time() - last_logged_time > 0.5:\n",
    "                last_logged_time = time.time()\n",
    "                # not sure what is going on here\n",
    "                pbar.set_postfix(phase = phase, **data)\n",
    "\n",
    "            self.learning_phase()\n",
    "\n",
    "        self.envs.close()\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [12:48<00:00,  1.27it/s, episode_duration=2.7, episode_length=500, episode_reward=500, phase=973]  \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>█████▅▄██▇▇████▄▄▇▆█▅▅▁▅▁▅█▇█▇█▆▃█▅▅▇▇▆▇</td></tr><tr><td>approx_kl</td><td>▁▁▂▄▃▂▃▄▁█▁▁▁▁▄▁▁▃▂▁▁▁▁▁▁▁▁▄▃▂▇▁▂▂▃▂▁▄▂▁</td></tr><tr><td>clip_frac</td><td>▁▁▂▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▂▃▃▁▁▁▁▂▅▄▁▁▁▅█▆</td></tr><tr><td>clipped_obj</td><td>▇▆▆▅▆▇▇▆▆▅▅▆▆▂▅▆▇▆▁▆▆▅▆▁▅▆▆▆▆▆▇▆▆▇▅▆▆█▆▅</td></tr><tr><td>entropy</td><td>█▇▇▆▆▅▅▅▆▆▆▆▅▆▆▅▆▆▆▆▅▆▆▆▆▆▆▆▅▄▂▄▃▂▃▃▂▂▁▁</td></tr><tr><td>episode_duration</td><td>▁▁▁▁▂▁▂▂▄▁▃▅▃▃▅▄▃▅▅▄▄▃▄▃▃▄▃▃▃▃▄▆▆█▆▅▅▅▅▅</td></tr><tr><td>episode_length</td><td>▁▁▁▁▁▁▁▃▄▄█▆▆▄▆█▃▃█▅▆▇▄▄▄▄▃▄▅▅▆▅▄▇██████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▁▁▂▄▄▅▄▅▃▇▄▄▅▄▆▄▅▄▄▆███████████████</td></tr><tr><td>lr</td><td>██▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▁▁▁▁</td></tr><tr><td>total_steps</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>value_fn_loss</td><td>▄▄▃▃▃▃▄▄▃▃▃▇█▃▆▃▃▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>values</td><td>▁▂▂▂▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▆▅▆▇▅████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>1042.401</td></tr><tr><td>approx_kl</td><td>4e-05</td></tr><tr><td>clip_frac</td><td>0</td></tr><tr><td>clipped_obj</td><td>9e-05</td></tr><tr><td>entropy</td><td>0.00301</td></tr><tr><td>episode_duration</td><td>2.70312</td></tr><tr><td>episode_length</td><td>500</td></tr><tr><td>episode_reward</td><td>500</td></tr><tr><td>lr</td><td>0.00023</td></tr><tr><td>total_steps</td><td>499712</td></tr><tr><td>value_fn_loss</td><td>0.0</td></tr><tr><td>values</td><td>99.99963</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CartPole-v1_Cartpole_PPO_seed1__20250630-220341</strong> at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/jvhe7pta' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/jvhe7pta</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 80 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_220345-jvhe7pta/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = PPOArgs(video_log_freq = 50)\n",
    "trainer = PPOTrainer(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [11:56<00:00,  1.36it/s, episode_duration=3.44, episode_length=500, episode_reward=488, phase=974]   \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>▇▇▆▇█▄████▃▅███▁▅█▅██████▂█████▅███▅█▇▄▅</td></tr><tr><td>approx_kl</td><td>▁▁▁▁▄▁▂▂▁▃▂▃▂▄▂▁▂▁▁▁██▂▄▂▃▄▂▅▃▂▃▂▁▃▁▂▃▃▃</td></tr><tr><td>clip_frac</td><td>▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁█▃▂▄▄▁▁▁▁▂▅▄▂▁▁</td></tr><tr><td>clipped_obj</td><td>▅▃▃▃▃▃▂▂▄▆▁▃▇▄▃▂▃█▆▄▃▃▃▂▃▄█▃▂▄▅▂▃▄█▃▂▄▃▃</td></tr><tr><td>entropy</td><td>▇██▇▇▆███▇█▇▇▇▇▆▆▇▆▇▅▄▅▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>episode_duration</td><td>▁▁▁▁▁▂▁▁▂▂▃▂▃▂▂▂▂▂▂▂▂▃▂▃▆█▆▇▇▆▆▆▇▆▆▆▆▆▆▇</td></tr><tr><td>episode_length</td><td>▁▂▁▁▁▁▂▄▁▁▂▂▂▂▃▂▁▂▂▃▃▂▃▂▄▂▃▃▃▃▃▃▃▅▃█████</td></tr><tr><td>episode_reward</td><td>▁▁▂▂▂▂▂▃▂▂▂▁▂▂▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▇▂▇▇▇█████</td></tr><tr><td>lr</td><td>██▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>total_steps</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>value_fn_loss</td><td>▆▅█▄▃▄▃▂▄▄▄▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>values</td><td>▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▃▃▃▆▇▇▇▇▇▇███████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>1017.15206</td></tr><tr><td>approx_kl</td><td>0.00051</td></tr><tr><td>clip_frac</td><td>0.00781</td></tr><tr><td>clipped_obj</td><td>0.00483</td></tr><tr><td>entropy</td><td>0.00165</td></tr><tr><td>episode_duration</td><td>3.4375</td></tr><tr><td>episode_length</td><td>500</td></tr><tr><td>episode_reward</td><td>488.39368</td></tr><tr><td>lr</td><td>0.00023</td></tr><tr><td>total_steps</td><td>499712</td></tr><tr><td>value_fn_loss</td><td>0.00152</td></tr><tr><td>values</td><td>97.48114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">reward_multiplicative</strong> at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/84a9mayi' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/84a9mayi</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 80 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_232626-84a9mayi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        x, velo, theta, omega = obs\n",
    "        theta_max = 0.2095\n",
    "        x_max = 0.2095\n",
    "        \n",
    "\n",
    "        reward_theta = 1 - abs(theta / theta_max)\n",
    "        reward_x = 1 - abs(x / x_max)\n",
    "\n",
    "        reward_new = (reward_theta + reward_x) / 2\n",
    "\n",
    "        return obs, reward_new, terminated, truncated, info\n",
    "\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)\n",
    "args = PPOArgs(env_id=\"EasyCart-v0\", video_log_freq=50)\n",
    "trainer = PPOTrainer(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alexwa/Documents/GitHub/rl/wandb/run-20250630_234501-26ep9mkt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/26ep9mkt' target=\"_blank\">EasyCart-v0_Cartpole_PPO_seed1__20250630-234501</a></strong> to <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/26ep9mkt' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/26ep9mkt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [11:40<00:00,  1.39it/s, episode_duration=2.67, episode_length=500, episode_reward=390, phase=975]    \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>▇▇▇▇▇▁▄▇▇▇▇▇▇▅▇▇█▇▃▄▇▃▇▇▇▄▆▆▇▇▇▇▇▇▇▇▇▇▅▇</td></tr><tr><td>approx_kl</td><td>▁▁▁▁▁▁▂▃▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂█▁▂▂▃▁▃▁▂▂▁▁▁▂▁▁▂</td></tr><tr><td>clip_frac</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▁▂▁▁▁▁▁▁▁█▁▁▁▁▁▁▂▁▅▂▁▁▂▅▁▃▂</td></tr><tr><td>clipped_obj</td><td>▂▁▁▂▁▅▃▂▄█▅▂▁▂▂▂▂▃▂▃▂▅▃▁▂▂▂▃▅▂▃▃▇▂▄▂▄▂▁▄</td></tr><tr><td>entropy</td><td>██▇▆▆▅▄▃▄▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▃▂▃▄▃▄▃▃▃▂▃▂▃▂▂▃</td></tr><tr><td>episode_duration</td><td>▃▁▁▂▁▂▁▂▂▃▂▁▂▂▂▃▂▂▁▃▃▃▂▃▅▂▄▅▄▅▃▄▃▅▃▇███▃</td></tr><tr><td>episode_length</td><td>▁▁▁▂▁▂▁▂▂▂▁▂▂▃▂▂▂▂▂▅▄▃▂▆▂▅▂▅▄▃▂▄▃▂▃████▄</td></tr><tr><td>episode_reward</td><td>▄▄▄▄▄▄▄▄▅▅▅▅▄▄▄▄▅▄▅▄▅▄▅▅▆▄▅▃▅▆▄▅▅█▃▄▄▃▁▇</td></tr><tr><td>lr</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>total_steps</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇███</td></tr><tr><td>value_fn_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂█▁▁▁▁▂▁▁▁▂▂▂▁▄</td></tr><tr><td>values</td><td>▂▂▂▂▂▃▂▂▄▄▁▂▅▂▃▄▄▄▃▂▃▁▃▂▃▃▄▅▃▄▇▆▇▅█▅▆▅▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>1019.86782</td></tr><tr><td>approx_kl</td><td>0.00361</td></tr><tr><td>clip_frac</td><td>0.07031</td></tr><tr><td>clipped_obj</td><td>-0.00575</td></tr><tr><td>entropy</td><td>0.00194</td></tr><tr><td>episode_duration</td><td>2.67188</td></tr><tr><td>episode_length</td><td>500</td></tr><tr><td>episode_reward</td><td>389.9183</td></tr><tr><td>lr</td><td>0.00023</td></tr><tr><td>total_steps</td><td>499712</td></tr><tr><td>value_fn_loss</td><td>4.9582</td></tr><tr><td>values</td><td>52.51336</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EasyCart-v0_Cartpole_PPO_seed1__20250630-234501</strong> at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/26ep9mkt' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/26ep9mkt</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 96 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_234501-26ep9mkt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        x, velo, theta, omega = obs\n",
    "        theta_max = 0.2095\n",
    "        x_max = 0.2095\n",
    "        \n",
    "\n",
    "        reward_theta = 1 - abs(theta / theta_max)\n",
    "        reward_x = 1 - abs(x / x_max)\n",
    "\n",
    "        reward_new = reward_theta * reward_x\n",
    "\n",
    "        return obs, reward_new, terminated, truncated, info\n",
    "\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)\n",
    "args = PPOArgs(env_id=\"EasyCart-v0\", video_log_freq=50)\n",
    "trainer = PPOTrainer(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/alexwa/Documents/GitHub/rl/wandb/run-20250701_001215-j7vztpzm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/j7vztpzm' target=\"_blank\">EasyCart-v0_Cartpole_PPO_seed1__20250701-001215</a></strong> to <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/j7vztpzm' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/j7vztpzm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [12:01<00:00,  1.35it/s, episode_duration=2.78, episode_length=500, episode_reward=478, phase=975] \n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>█▇▄▅▆▄▁▇███████▅▅▅▆▁█▆▆█▇▇▇███▇▆▇▇▇▇▇█▇█</td></tr><tr><td>approx_kl</td><td>▁▁▁▃▁▁▁▁▁▁▁▁▁▃▁▁▁▁▂▂▁▂▄█▂▁█▃▂▂▂▂▄▁▁▄▂▁▁▁</td></tr><tr><td>clip_frac</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▁▁▂█▄▁▁▂▂▁▂▄▃▅▅▄▁▄▃▁▃▆▄▃▃</td></tr><tr><td>clipped_obj</td><td>▅▄▄▅▄▄▆▆▃▅▄█▃▂▂▅▆▄▅▄▄▅▄▅▆▆▄█▅▃▂▆▃▃▂▁▄▄▄▅</td></tr><tr><td>entropy</td><td>██▇▆▆▆▆▆▆▆▆▆▅▅▆▅▄▄▄▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁</td></tr><tr><td>episode_duration</td><td>▁▁▁▁▂▃▂▂▂▃▅▃█▇▇███▆▆▇█▆▇▇▇▆▇▇▆▇▇▆▇▇▆▆▆▇▇</td></tr><tr><td>episode_length</td><td>▁▁▁▃▂▃▂▁▂▁▁▃▅▆▃▇▄▃▆▅▆███████████████████</td></tr><tr><td>episode_reward</td><td>▁▁▁▁▁▂▂▁▁▁▁▂▁▁▂▂▂▃▂▇▇██▇▇███████████████</td></tr><tr><td>lr</td><td>██▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>total_steps</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇██</td></tr><tr><td>value_fn_loss</td><td>▆▆▆▂▂▅█▃▇▄▃▄▃▁▂▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>values</td><td>▁▂▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇███████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>937.327</td></tr><tr><td>approx_kl</td><td>0.00136</td></tr><tr><td>clip_frac</td><td>0.00781</td></tr><tr><td>clipped_obj</td><td>0.00096</td></tr><tr><td>entropy</td><td>0.00148</td></tr><tr><td>episode_duration</td><td>2.78125</td></tr><tr><td>episode_length</td><td>500</td></tr><tr><td>episode_reward</td><td>477.97803</td></tr><tr><td>lr</td><td>0.00023</td></tr><tr><td>total_steps</td><td>499712</td></tr><tr><td>value_fn_loss</td><td>0.006</td></tr><tr><td>values</td><td>97.01659</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EasyCart-v0_Cartpole_PPO_seed1__20250701-001215</strong> at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/j7vztpzm' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/j7vztpzm</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 48 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250701_001215-j7vztpzm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        x, velo, theta, omega = obs\n",
    "        theta_max = 0.2095\n",
    "        x_max = 0.2095\n",
    "        \n",
    "        # Clip the ratios to ensure they don't exceed 1\n",
    "        theta_ratio = np.clip(abs(theta / theta_max), 0, 1)\n",
    "        x_ratio = np.clip(abs(x / x_max), 0, 1)\n",
    "        \n",
    "        # This ensures rewards are always in [0, 1]\n",
    "        reward_theta = 1 - theta_ratio\n",
    "        reward_x = 1 - x_ratio\n",
    "        \n",
    "        # Now the product will always be non-negative and we can safely take the square root\n",
    "        reward_new = np.sqrt(max(0, reward_theta * reward_x))\n",
    "\n",
    "        return obs, reward_new, terminated, truncated, info\n",
    "\n",
    "# Re-register with a new version to ensure we're using the updated environment\n",
    "if \"EasyCart-v0\" in gym.envs.registry:\n",
    "    del gym.envs.registry[\"EasyCart-v0\"]\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)\n",
    "args = PPOArgs(env_id=\"EasyCart-v0\", video_log_freq=50)\n",
    "trainer = PPOTrainer(args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 976/976 [11:42<00:00,  1.39it/s, episode_duration=2.77, episode_length=500, episode_reward=499, phase=973]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 40 that is less than the current step 512. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 60 that is less than the current step 513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 80 that is less than the current step 513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 112 that is less than the current step 513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 144 that is less than the current step 513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 148 that is less than the current step 513. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 204 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 208 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 232 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 248 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 328 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 340 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 376 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 396 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 424 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 432 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 452 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 484 that is less than the current step 514. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 496 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 504 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 512 that is less than the current step 515. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>▁▆▅▆▇▇▇▇▇▇▆▇▃▆▇▇▇▇▆▇▇▇▇▇▇▇█▇▇▇▆▆▇▆▇▃▇▇▇▇</td></tr><tr><td>approx_kl</td><td>▂▅▆▃▃▁▁▂▁▃▁▁▁▁▁▁▁▂▂▁▆▂▆▁▁▁▂▂▁▃▃▁▁▁▁▁▁▅▂█</td></tr><tr><td>clip_frac</td><td>▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅█▁▂▁▁▁▁▁▁▁▁▁▄</td></tr><tr><td>clipped_obj</td><td>▆▄▃▄▁▃▂▂▂▄▂▂▃▇▃▃▃▂▇▁▄▃▄▂▄▁▆▃▃▃▃▃▃▄▅█▃▃▄▃</td></tr><tr><td>entropy</td><td>██▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▆▄▅▄▄▄▄▅▄▄▅▂▃▂▂▂▁▂▂▁▁</td></tr><tr><td>episode_duration</td><td>▁▁▁▁▁▂▁▂▂▂▆▇▅▆▅▅▆▇▅▆▇▇▇▄▅▄▅▅▃▅▃▃▃▅█▅▅▅█▇</td></tr><tr><td>episode_length</td><td>▁▂▁▁▂▆▂█▇██▄▂▅███▃██▅▃▃▄█▄▃▄████▇▅█▇████</td></tr><tr><td>episode_reward</td><td>▂▁▁▂▁▂▃▅▆▆█▃▄▃█▅▃████▅▃▃▃▄▃▃███▇▅▇▄██▇██</td></tr><tr><td>lr</td><td>█████▇▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>total_steps</td><td>▁▁▁▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>value_fn_loss</td><td>▃▃▃▃▂▄▂▂▂▃▄▃▂▁▃▁▁▁▁█▅▆▇▆▅▂▁▁▁▁▁▅▃▁▁▁▁▁▁▁</td></tr><tr><td>values</td><td>▁▁▁▁▂▂▃▃▄▄▄▄▄▄▄▄▅▅▇█▇▇▇▆▆▇▇███▇█▅▆▄▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>1028.71546</td></tr><tr><td>approx_kl</td><td>0.00449</td></tr><tr><td>clip_frac</td><td>0.07031</td></tr><tr><td>clipped_obj</td><td>0.01667</td></tr><tr><td>entropy</td><td>0.00181</td></tr><tr><td>episode_duration</td><td>2.76562</td></tr><tr><td>episode_length</td><td>500</td></tr><tr><td>episode_reward</td><td>498.58557</td></tr><tr><td>lr</td><td>0.00023</td></tr><tr><td>total_steps</td><td>499712</td></tr><tr><td>value_fn_loss</td><td>7e-05</td></tr><tr><td>values</td><td>99.69046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">EasyCart-v0_Cartpole_PPO_seed1__20250630-235645</strong> at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/4u70g9jw' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO/runs/4u70g9jw</a><br> View project at: <a href='https://wandb.ai/djdumpling-yale/Cartpole_PPO' target=\"_blank\">https://wandb.ai/djdumpling-yale/Cartpole_PPO</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 59 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_235645-4u70g9jw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        x, velo, theta, omega = obs\n",
    "        theta_max = 0.2095\n",
    "        x_max = 0.2095\n",
    "        \n",
    "\n",
    "        kinetic_energy = 0.5 * (velo ** 2 + omega ** 2)\n",
    "        reward_new = 1 - 0.1 * kinetic_energy\n",
    "\n",
    "        return obs, reward_new, terminated, truncated, info\n",
    "\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)\n",
    "args = PPOArgs(env_id=\"EasyCart-v0\", video_log_freq=50)\n",
    "trainer = PPOTrainer(args)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
