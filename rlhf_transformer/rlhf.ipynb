{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66240717",
   "metadata": {},
   "source": [
    "Still need to run full run once GPU config + connect is figured out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668b0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import jaxtyping\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Callable\n",
    "\n",
    "import einops\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import wandb\n",
    "import tqdm\n",
    "import tabulate\n",
    "from eindex import eindex\n",
    "from jaxtyping import Float, Int\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from tabulate import tabulate\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = t.device(\"mps\" if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26693f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RLHFArgs:\n",
    "    # random\n",
    "    seed: int = 1\n",
    "\n",
    "    # logging\n",
    "    wandb_project_name: str = \"rlhf_transformers\"\n",
    "    wandb_entity: str | None = None\n",
    "\n",
    "    # macro-training \n",
    "    total_phases: int = 200\n",
    "    batch_size: int = 32 #enforce batch_size % num_minibatches == 0\n",
    "    num_minibatches: int = 4\n",
    "    batches_per_learning_phase: int = 2\n",
    "\n",
    "    # optimization hyperparameters\n",
    "    base_lr: float = 2e-5\n",
    "    head_lr: float = 5e-4\n",
    "    max_grad_norm: float = 1.0\n",
    "    warmup_steps: int = 20 #enforce warmup_steps < total_phases\n",
    "    final_scale: float = 0.1\n",
    "\n",
    "    # PPO objective function coefficients\n",
    "    clip_coef: float = 0.2\n",
    "    vf_coef: float = 0.15\n",
    "    ent_coef: float = 0.001\n",
    "\n",
    "    # model and sampling with prefix\n",
    "    base_model: str = \"gpt2-medium\"\n",
    "    gen_len: int = 50\n",
    "    temperature: float = 1.0\n",
    "    top_k: int = 10\n",
    "    prefix: str = \"This movie was really\"\n",
    "    prepend_bos: bool = True\n",
    "\n",
    "    # RLHF-specific arguments\n",
    "    kl_coef: float = 2.5\n",
    "    reward_fn: Callable = lambda x: 0.0\n",
    "    normalize_reward: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.minibatch_size = self.batch_size // self.num_minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49a83df",
   "metadata": {},
   "source": [
    "# Setup: working with the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e06f9",
   "metadata": {},
   "source": [
    "Right after the last layernorm before we unembed our tokens, we add a hook function (our value head) which computes a **value estimate** for the generated sequence. The hook function is a simple 2-layer neural network which computs the value estimate during the forward pass and stores it externally.\n",
    "\n",
    "Why do we choose this location? After the layernorm essentially normalizes the reward, and before the unembedding because we take in the enumerated tokens as input. It is also towards the end because (supposedly) it contains the most information after accumulating through the residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d11d5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "class TransformerWithValueHead(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = HookedTransformer.from_pretrained(base_model)\n",
    "        \n",
    "        d_model = self.base_model.cfg.d_model\n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, 1))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        value_head_output = None\n",
    "\n",
    "        # resid_post: [batch seq d_model] so\n",
    "        # value_head_ouput: [batch seq]\n",
    "        def calc_and_store_value_head_output(resid_post, hook):\n",
    "            # nonlocal: for variables inside nested functions\n",
    "            nonlocal value_head_output\n",
    "            value_head_output = self.value_head(resid_post).squeeze(-1)\n",
    "\n",
    "        # run_with_hooks injects parameters\n",
    "        logits = self.base_model.run_with_hooks(\n",
    "            input_ids,\n",
    "            return_type = \"logits\",\n",
    "            # \"normalized\" to represent being after the LayerNorm\n",
    "            fwd_hooks = [(utils.get_act_name(\"normalized\"), calc_and_store_value_head_output)])\n",
    "        \n",
    "        return logits, value_head_output\n",
    "    \n",
    "model = TransformerWithValueHead(\"gpt2-small\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74bc51",
   "metadata": {},
   "source": [
    "Defaulting `stop_at_eos = False` is interesting. From an interpretability perspective, `stop_at_eos = False`  helps with seeing hallucations. From a training perspective, it helps measure how well the model learned to stop and enables models to learn from full length text, not truncated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dddcb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend_bos: appending a BOS token at the start of a sequence, which marks the start\n",
    "def get_samples(base_model, prompt, batch_size, gen_len, temperature, top_k, prepend_bos):\n",
    "    # returns one tokenized prompt, squeeze to extract pure tokens\n",
    "    input_ids = base_model.to_tokens(prompt, prepend_bos = prepend_bos).squeeze(0)\n",
    "\n",
    "    output_ids = base_model.generate(\n",
    "        # [tokens] becomes [batch_size tokens]\n",
    "        # repeats input_ids once batch_size times\n",
    "        input_ids.repeat(batch_size, 1), \n",
    "        max_new_tokens = gen_len, \n",
    "        stop_at_eos = False,\n",
    "        temperature = temperature,\n",
    "        top_k = top_k, \n",
    "        verbose = False\n",
    "    )\n",
    "\n",
    "    # samples: [batch_size sequence]\n",
    "    samples = base_model.to_string(output_ids)\n",
    "\n",
    "    # .clone() to prevent modification to internal output_ids\n",
    "    return output_ids.clone(), samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb89e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Token IDs                                              </span>┃<span style=\"font-weight: bold\"> Samples                                                </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ [1212, 3807, 373, 1107, 1257, 284, 2342, 13, 632, 373, │ 'This movie was really fun to watch. It was really fun │\n",
       "│ 1107, 1257, 284, 766, 477, 777, 3435, 11, 290]         │ to see all these characters, and'                      │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1257, 284, 2342, 11, 314, 550, │ 'This movie was really fun to watch, I had a lot of    │\n",
       "│ 257, 1256, 286, 1257, 351, 340, 11, 475, 314]          │ fun with it, but I'                                    │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 2089, 290, 257, 1256, 286,     │ 'This movie was really bad and a lot of people were    │\n",
       "│ 661, 547, 1107, 6507, 553, 531, 530, 286, 262, 661]    │ really sad,\" said one of the people'                   │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1049, 290, 314, 1101, 1107,    │ 'This movie was really great and I\\'m really happy     │\n",
       "│ 3772, 326, 262, 28303, 3066, 284, 466, 428, 553, 531]  │ that the filmmakers decided to do this,\" said'         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 655, 530, 286, 883, 7328, 314, │ \"This movie was really just one of those films I never │\n",
       "│ 1239, 765, 284, 766, 757, 13, 632, 338, 257]           │ want to see again. It's a\"                             │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mToken IDs                                             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSamples                                               \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ [1212, 3807, 373, 1107, 1257, 284, 2342, 13, 632, 373, │ 'This movie was really fun to watch. It was really fun │\n",
       "│ 1107, 1257, 284, 766, 477, 777, 3435, 11, 290]         │ to see all these characters, and'                      │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1257, 284, 2342, 11, 314, 550, │ 'This movie was really fun to watch, I had a lot of    │\n",
       "│ 257, 1256, 286, 1257, 351, 340, 11, 475, 314]          │ fun with it, but I'                                    │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 2089, 290, 257, 1256, 286,     │ 'This movie was really bad and a lot of people were    │\n",
       "│ 661, 547, 1107, 6507, 553, 531, 530, 286, 262, 661]    │ really sad,\" said one of the people'                   │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 1049, 290, 314, 1101, 1107,    │ 'This movie was really great and I\\'m really happy     │\n",
       "│ 3772, 326, 262, 28303, 3066, 284, 466, 428, 553, 531]  │ that the filmmakers decided to do this,\" said'         │\n",
       "├────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────┤\n",
       "│ [1212, 3807, 373, 1107, 655, 530, 286, 883, 7328, 314, │ \"This movie was really just one of those films I never │\n",
       "│ 1239, 765, 284, 766, 757, 13, 632, 338, 257]           │ want to see again. It's a\"                             │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_ids, samples = get_samples(\n",
    "    model.base_model,\n",
    "    prompt = \"This movie was really\",\n",
    "    batch_size = 5,\n",
    "    gen_len = 15,\n",
    "    temperature = 0.8,\n",
    "    top_k = 15,\n",
    "    prepend_bos = False\n",
    ")\n",
    "\n",
    "table = Table(\"Token IDs\", \"Samples\", show_lines = True)\n",
    "for ids, sample in zip(sample_ids, samples):\n",
    "    # ids.tolist(): convert Tensor into Python list\n",
    "    # repr(sample): printable representation (adds single quotes)\n",
    "    table.add_row(str(ids.tolist()), repr(sample))\n",
    "\n",
    "rprint(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae35443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .half(): uses float16 precision for faster inference on GPUs\n",
    "cls_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\").half().to(device)\n",
    "cls_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "\n",
    "def reward_fn_sentiment_imdb(gen_sample, direction: str = \"pos\"):\n",
    "    # \"pt\" for pytorch tensors, padding + truncation to ensure same length generation\n",
    "    tokens = cls_tokenizer(gen_sample, return_tensors = \"pt\", padding = True, truncation = True)[\"input_ids\"].to(device)\n",
    "    # logits: [batch_size, 2] for pos/neg classification\n",
    "    logits = cls_model(tokens).logits\n",
    "    # positive_cls: [batch_size] contains relevant class after softmaxing to get probabilities\n",
    "    positive_cls = logits.softmax(-1)[:, 1 if (direction == \"pos\") else 0]\n",
    "    return positive_cls.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18147c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_reward(reward, eps = 1e-5):\n",
    "    return (reward - reward.mean()) / (reward.std() + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b53665",
   "metadata": {},
   "source": [
    "Using the simple $A(s_t, a_t) = Q(s_t, a_t) - V(s_t)$ formula where $Q(s_t, a_t)$ is based off of the one-step Q estimates. if $t<T$, then our Q estimate is $V(s_{t+1})$, but if $t=T$, then we can use the known reward $r_t$ for the entire sequence.\n",
    "\n",
    "GAE is an alternative but wouldn't bring a significant improvement since GAE is most helpful in reducing variance in advantage estimation, and our situation is low variance (each step adds a single token to our sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c32d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(values, rewards, prefix_len):\n",
    "    one_step_est = t.cat([values[:, prefix_len:-1], rewards[:, None]], dim = -1)\n",
    "    zero_step_est = values[:,  prefix_len-1:-1]\n",
    "    return one_step_est - zero_step_est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a52786",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc28b7",
   "metadata": {},
   "source": [
    "Compared to the PPO implementation, there are a few differences in `ReplayMemory`. \n",
    "- Don't need an `add` function because we add it all at once instead of one-by-one.\n",
    "- Don't need multiple environments\n",
    "\n",
    "And for `ReplayMinibatch`\n",
    "- Don't need `actions` anymore since there isn't a sense of an \"agent\" since actions (tokens generated) are contained within the sequences\n",
    "- Don't need `dones` since we set the sequence to be `gen_len` long\n",
    "- Sotre `ref_logits` as a part of the KL penalty w.r.t the reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcb2df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ReplayMinibatch:\n",
    "    sample_ids: Float[Tensor, \"minibatch_size seq_len\"]\n",
    "    logprobs: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    advantages: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    returns: Float[Tensor, \"minibatch_size gen_len\"]\n",
    "    ref_logits: Float[Tensor, \"minibatch_size seq_len d_vocab\"]\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, args, sample_ids, logprobs, advantages, values, ref_logits):\n",
    "        self.args = args\n",
    "        self.sample_ids = sample_ids\n",
    "        self.logprobs = logprobs\n",
    "        self.advantages = advantages\n",
    "        self.values = values\n",
    "        self.ref_logits = ref_logits\n",
    "\n",
    "    def get_minibatches(self):\n",
    "        minibatches = []\n",
    "\n",
    "        # Detach tensors to avoid retaining computation graph and causing double-backward errors\n",
    "        sample_ids = self.sample_ids.detach() if hasattr(self.sample_ids, \"detach\") else self.sample_ids\n",
    "        logprobs = self.logprobs.detach() if hasattr(self.logprobs, \"detach\") else self.logprobs\n",
    "        advantages = self.advantages.detach() if hasattr(self.advantages, \"detach\") else self.advantages\n",
    "        values = self.values.detach() if hasattr(self.values, \"detach\") else self.values\n",
    "        ref_logits = self.ref_logits.detach() if hasattr(self.ref_logits, \"detach\") else self.ref_logits\n",
    "\n",
    "        # since we use 1-step advantage estimation\n",
    "        # returns = next-step estimate of value function\n",
    "        returns = advantages + values[:, -self.args.gen_len - 1: -1]\n",
    "\n",
    "        for _ in range(self.args.batches_per_learning_phase):\n",
    "            for indices in t.randperm(self.args.batch_size).reshape(self.args.num_minibatches, -1):\n",
    "                minibatches.append(ReplayMinibatch(\n",
    "                    sample_ids = sample_ids[indices],\n",
    "                    logprobs=logprobs[indices],\n",
    "                    advantages=advantages[indices],\n",
    "                    returns=returns[indices],\n",
    "                    ref_logits=ref_logits[indices]\n",
    "                ))\n",
    "\n",
    "        return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907caad2",
   "metadata": {},
   "source": [
    "In addition to the 3 components of the total PPO objective, we'll add on the KL penalty as a part of the RLHF framework.\n",
    "- The KL prediction shift penalty is $-\\lambda_{KL} D_{KL}(\\pi_{PPO}\\phantom{.}|| \\phantom{.}\\pi_{base})$ (and not the other way) because the penalization should be for results that are likely under $\\pi_{PPO}$ and unlikely under $\\pi_{base}$. Expanding the KL penalty yields: $$\\lambda_{KL} \\cdot \\sum_i \\pi_{PPO_i}\\log\\left(\\frac{\\pi_{PPO_i}}{\\pi_{base_i}}\\right)$$\n",
    "- The `entropy`, `value_fn`, and `clipped_sur_obj` functions are essentially the same from PPO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb43bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .mean() to aggregate over the batch + stabilize training\n",
    "def calc_kl_penalty(logits, ref_logits, kl_coef):\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    ref_log_probs = ref_logits.log_softmax(-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    kl_div = (probs * (log_probs - ref_log_probs)).sum(-1)\n",
    "\n",
    "    return kl_coef * kl_div.mean()\n",
    "\n",
    "def calc_entropy_bonus(logits, ent_coef):\n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    probs = log_probs.exp()\n",
    "\n",
    "    entropy = -(log_probs * probs).sum(-1)\n",
    "\n",
    "    return ent_coef * entropy.mean()\n",
    "\n",
    "# supervised regression loss for the value function\n",
    "def calc_value_fn_loss(values, mb_returns, vf_coef):\n",
    "    return 1/2 * vf_coef * (values - mb_returns).pow(2).mean()\n",
    "\n",
    "def calc_clipped_sur_obj(logprobs, mb_logprobs, mb_advantages, clip_coef, eps = 1e-8):\n",
    "    logits_diff = logprobs - mb_logprobs\n",
    "    # ratio of the policies\n",
    "    ratio = t.exp(logits_diff)\n",
    "\n",
    "    # normalizing the advantages\n",
    "    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + eps)\n",
    "\n",
    "    # standard clip application\n",
    "    non_clipped = ratio * mb_advantages\n",
    "    clipped = t.clip(ratio, 1 - clip_coef, 1 + clip_coef) * mb_advantages\n",
    "\n",
    "    return t.minimum(non_clipped, clipped).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f41984",
   "metadata": {},
   "source": [
    "`get_log_probs` ensures that the output is always of size `(minibatch_size, gen_len)`. We only care about the log probs of the tokens generated, not in the prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e760a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(logits, tokens, prefix_len):\n",
    "    if prefix_len is not None:\n",
    "        logits = logits[:, prefix_len-1:]\n",
    "        tokens = tokens[:, prefix_len-1:]\n",
    "    \n",
    "    log_probs = logits.log_softmax(-1)\n",
    "    shaped_log_probs = eindex(log_probs, tokens, \"b s [b s+1]\")\n",
    "\n",
    "    return shaped_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2e8f1",
   "metadata": {},
   "source": [
    "For both the base model and the value head, we define seperate learning rates, which makes sense since the value head is randomly initalized whereas the base model is already built out.\n",
    "\n",
    "For the scheduler, we use a lienar warmup up to `1.0` then linear decay down to `args.final_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94c881ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, base_lr, head_lr):\n",
    "    return t.optim.AdamW(\n",
    "        [\n",
    "           {\"params\": model.base_model.parameters(), \"lr\": base_lr},\n",
    "           {\"params\": model.value_head.parameters(), \"lr\": head_lr} \n",
    "        ], maximize = True)\n",
    "\n",
    "def get_optimizer_and_scheduler(args, model):\n",
    "    def lr_lambda(step):\n",
    "        if step < args.warmup_steps:\n",
    "            return step / args.warmup_steps\n",
    "        else:\n",
    "            return 1 - (1 - args.final_scale) * (step - args.warmup_steps) / (args.total_phases - args.warmup_steps)\n",
    "        \n",
    "    optimizer = get_optimizer(model, args.base_lr, args.head_lr)\n",
    "    scheduler = t.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lr_lambda)\n",
    "\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffef61",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53d9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLHFTrainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.run_name = f\"{args.wandb_project_name}_{args.base_model}_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "        self.model = TransformerWithValueHead(args.base_model).to(device).train()\n",
    "        self.ref_model = HookedTransformer.from_pretrained(args.base_model).to(device).eval()\n",
    "        self.optimizer, self.scheduler = get_optimizer_and_scheduler(self.args, self.model)\n",
    "        self.prefix_len = len(self.model.base_model.to_str_tokens(self.args.prefix, prepend_bos = self.args.prepend_bos))\n",
    "\n",
    "    def compute_rlhf_objective(self, minibatch):\n",
    "        logits, values = self.model(minibatch.sample_ids)\n",
    "        log_probs = get_log_probs(logits, minibatch.sample_ids, self.prefix_len)\n",
    "\n",
    "        gen_len_slice = slice(-self.args.gen_len - 1, -1)\n",
    "\n",
    "        # ??? understand the slicing and why it differs for minibatch and log_probs\n",
    "        kl_penalty = calc_kl_penalty(logits[:, gen_len_slice], minibatch.ref_logits[:, gen_len_slice], self.args.kl_coef)\n",
    "        entropy = calc_entropy_bonus(logits[:, gen_len_slice], self.args.ent_coef)\n",
    "        value_fn_loss = calc_value_fn_loss(values[:, gen_len_slice], minibatch.returns, self.args.vf_coef)\n",
    "        clipped_sur_obj = calc_clipped_sur_obj(log_probs, minibatch.logprobs, minibatch.advantages, self.args.clip_coef)\n",
    "\n",
    "        ppo_obj_fn = clipped_sur_obj - value_fn_loss + entropy\n",
    "        total_obj_fn = ppo_obj_fn - kl_penalty\n",
    "\n",
    "        # RL-specific logging\n",
    "        with t.inference_mode():\n",
    "            logratio = log_probs - minibatch.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            clipfracs = [((ratio - 1.0).abs() > self.args.clip_coef).float().mean().item()]\n",
    "        wandb.log(\n",
    "            dict(\n",
    "                total_steps=self.step,\n",
    "                lr=self.scheduler.get_last_lr()[0],\n",
    "                clipped_surrogate_objective=clipped_sur_obj.item(),\n",
    "                clipfrac=np.mean(clipfracs),\n",
    "                value_loss=value_fn_loss.item(),\n",
    "                values=values.mean().item(),\n",
    "                entropy_bonus=entropy.item(),\n",
    "                kl_penalty=kl_penalty.item(),\n",
    "            ),\n",
    "            step=self.step,\n",
    "        )\n",
    "\n",
    "        return total_obj_fn\n",
    "    \n",
    "    def rollout_phase(self):\n",
    "        sample_ids, samples = get_samples(\n",
    "            base_model = self.model.base_model,\n",
    "            prompt = self.args.prefix,\n",
    "            batch_size = self.args.batch_size,\n",
    "            gen_len = self.args.gen_len,\n",
    "            temperature = self.args.temperature,\n",
    "            top_k = self.args.top_k,\n",
    "            prepend_bos = self.args.prepend_bos)\n",
    "        \n",
    "        with t.inference_mode():\n",
    "            logits, values = self.model(sample_ids)\n",
    "            ref_logits = self.ref_model(sample_ids)\n",
    "\n",
    "        log_probs = get_log_probs(logits, sample_ids, self.prefix_len)\n",
    "\n",
    "        rewards = self.args.reward_fn(samples)\n",
    "        rewards_mean = rewards.mean().item()\n",
    "        rewards_normed = normalize_reward(rewards) if self.args.normalize_reward else rewards\n",
    "        advantages = compute_advantages(values, rewards_normed, self.prefix_len)\n",
    "\n",
    "        wandb.log({\"Mean Reward\": rewards_mean}, step = self.step)\n",
    "\n",
    "        # visualization\n",
    "        n_log_samples = min(3, self.args.batch_size)\n",
    "        ref_logprobs = get_log_probs(ref_logits[:n_log_samples], sample_ids[:n_log_samples], self.prefix_len).sum(-1)\n",
    "        headers = [\"Reward\", \"Ref logprobs\", \"Sample\"]\n",
    "        table_data = [[f\"{r:.4f}\", f\"{lp:.2f}\", repr(s)] for r, lp, s in zip(rewards.tolist(), ref_logprobs, samples)]\n",
    "        table = tabulate(table_data, headers, tablefmt=\"simple_grid\", maxcolwidths=[None, None, 90])\n",
    "        print(f\"Phase {self.phase+1:03}/{self.args.total_phases:03}, Mean reward: {rewards_mean:.4f}\\n{table}\\n\")\n",
    "\n",
    "        ref_logprobs_mean = ref_logprobs.mean().item()\n",
    "        wandb.log({\"Mean Ref logprobs\": ref_logprobs_mean}, step = self.step)\n",
    "\n",
    "        return ReplayMemory(\n",
    "            args = self.args,\n",
    "            sample_ids = sample_ids,\n",
    "            logprobs = log_probs,\n",
    "            advantages = advantages,\n",
    "            values = values,\n",
    "            ref_logits = ref_logits)\n",
    "        \n",
    "    def learning_phase(self, memory):\n",
    "        for minibatch in tqdm(memory.get_minibatches(), desc = f\"Learning phase {self.phase+1}\"):\n",
    "            self.optimizer.zero_grad()\n",
    "            total_obj_fn = self.compute_rlhf_objective(minibatch)\n",
    "            total_obj_fn.backward()\n",
    "            # clip according to max_norm\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), max_norm = self.args.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "            self.step += 1\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "    def train(self):\n",
    "        # ??? why define these here instead of global\n",
    "        self.step = 0\n",
    "        self.samples = []\n",
    "\n",
    "        wandb.init(\n",
    "            project = self.args.wandb_project_name,\n",
    "            entity = self.args.wandb_entity,\n",
    "            name = self.run_name,\n",
    "            config = self.args,\n",
    "        )\n",
    "\n",
    "        for self.phase in tqdm(range(self.args.total_phases), desc = \"Training phases\"):\n",
    "            memory = self.rollout_phase()\n",
    "            self.learning_phase(memory)\n",
    "\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ea683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with kl_coef = 0.0, it has no incentives to match the ref distribution, only maximize reward\n",
    "# args = RLHFArgs(kl_coef=0.0, total_phases=5, warmup_steps=0, reward_fn=reward_fn_sentiment_imdb)\n",
    "# trainer = RLHFTrainer(args)\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c450c9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = RLHFArgs(reward_fn = reward_fn_sentiment_imdb)\n",
    "trainer = RLHFTrainer(args)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
