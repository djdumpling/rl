{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import wandb\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "\n",
    "from gymnasium.spaces import Box, Discrete\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import nn, Tensor\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = t.device(\"mps\") if t.backends.mps.is_available() else \"cuda\" if t.cuda.is_available() else t.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\") # Discrete(2), left or right action\n",
    "print(f\"Observation space: {env.observation_space}\") # Box(4), position, velocity, angle, angular velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the QNetwork with a simple 3-layer NN with 10k parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 10934\n"
     ]
    }
   ],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_obs, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_obs, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, num_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "net = QNetwork(num_obs = 4, num_actions = 2)\n",
    "num_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the replay buffer. Thought the add function was neat since it slices off old elements. \n",
    "Since using mps, need to tensorify the 5 returned arrays.\n",
    "\n",
    "I'm curious much the capacity affects the rate of catastrophic forgetting. Maybe exponential decay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    rng: np.random.Generator\n",
    "\n",
    "    def __init__(self, obs_shape, action_shape, capacity, seed):\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "        self.capacity = capacity\n",
    "        self.seed = seed\n",
    "\n",
    "        # obs, actions, rewards, next_obs, terminated\n",
    "        self.obs = np.empty((0, *self.obs_shape), dtype = np.float32)\n",
    "        self.actions = np.empty((0, *self.action_shape), dtype = np.float32)\n",
    "        self.rewards = np.empty(0, dtype = np.float32)\n",
    "        self.next_obs = np.empty((0, *self.obs_shape), dtype = np.float32)\n",
    "        self.terminated = np.empty(0, dtype = bool)\n",
    "\n",
    "    def add(self, obs, actions, rewards, next_obs, terminated):\n",
    "        self.obs = np.concatenate((self.obs, obs))[-self.capacity:]\n",
    "        self.actions = np.concatenate((self.actions, actions))[-self.capacity:]\n",
    "        self.rewards = np.concatenate((self.rewards, rewards))[-self.capacity:]\n",
    "        self.next_obs = np.concatenate((self.next_obs, next_obs))[-self.capacity:]\n",
    "        self.terminated = np.concatenate((self.terminated, terminated))[-self.capacity:]\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        indices = self.rng.integers(0, self.capacity, size = batch_size)\n",
    "\n",
    "        obs_tensor = t.tensor(self.obs[indices], dtype = t.float32, device = device)\n",
    "        actions_tensor = t.tensor(self.actions[indices], dtype = t.float32, device = device)\n",
    "        rewards_tensor = t.tensor(self.rewards[indices], dtype = t.float32, device = device)\n",
    "        next_obs_tensor = t.tensor(self.next_obs[indices], dtype = t.float32, device = device)\n",
    "        terminated_tensor = t.tensor(self.terminated[indices], dtype = t.bool, device = device)\n",
    "\n",
    "        return obs_tensor, actions_tensor, rewards_tensor, next_obs_tensor, terminated_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only made sense to me mathetically until I realized it's analagous to ReLU being max(0,x)\n",
    "\n",
    "Still don't exactly understand what's going on behind the scenes with `.detach().cpu().numpy()`, will need to dig a little deeper into the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(curr_step, start_e, end_e, exploration_fraction, total_timesteps):\n",
    "    return start_e + (end_e - start_e) * min(curr_step / (exploration_fraction * total_timesteps), 1)\n",
    "\n",
    "# returns the sampled action for each env\n",
    "def epsilon_greedy_policy(envs, q_net, obs, epsilon):\n",
    "    obs = t.from_numpy(obs).float()\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.Generator.integers(0, envs.single_action_space.n)\n",
    "    else:\n",
    "        q_values = q_net(obs)\n",
    "        return q_values.argmax(dim = 1).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining standard arguments for a DQN, for global, wandb, durations, hyperparameters, and rl-specific stuff.\n",
    "Learned that `@dataclass` is for specific for classes that holds memory, automatically initializes stuff like `def __init__()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DQNArgs:\n",
    "    \n",
    "    seed = 0\n",
    "    env_id = \"CartPole\"\n",
    "\n",
    "    wandb_project_name = 'DQN CartPole'\n",
    "    wandb_entity = None\n",
    "    video_log_freq = 50\n",
    "\n",
    "    total_timesteps = 1e6\n",
    "    steps_per_train = 1e1\n",
    "    trains_per_target_update = 1e2\n",
    "    buffer_size = 1e4\n",
    "\n",
    "    batch_size = 128\n",
    "    learning_rate = 2.5e-4\n",
    "\n",
    "    gamma = 0.99\n",
    "    start_e = 1.0\n",
    "    end_e = 0.1\n",
    "    exploration_fraction = 0.2\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.total_training_steps = (self.total_timesteps - self.buffer_size) // self.steps_per_train\n",
    "\n",
    "args = DQNArgs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard implementation of our DQN Agent. We use true_next_obs, an augmented version of our next_obs with the information of whether we are terminated or truncated. Every single step, we add to the buffer and reset our observation, ready for the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, envs, buffer, q_network, start_e, end_e, exploration_fraction, total_timesteps):\n",
    "        self.envs = envs\n",
    "        self.buffer = buffer\n",
    "        self.q_network = q_network\n",
    "        self.start_e = start_e\n",
    "        self.end_e = end_e\n",
    "        self.exploration_fraction = exploration_fraction\n",
    "        self.total_timesteps = total_timesteps\n",
    "        \n",
    "        self.step = 0\n",
    "        self.obs, _ = envs.reset()\n",
    "        self.epsilon = start_e\n",
    "\n",
    "    def get_actions(self, obs):\n",
    "        self.epsilon = linear_schedule(self.start_e, self.end_e, self.exploration_fraction, self.total_timesteps)\n",
    "        actions = epsilon_greedy_policy(self.envs, self.q_network, self.obs, self.epsilon)\n",
    "        return actions\n",
    "\n",
    "    def play_step(self):\n",
    "        self.obs = np.array(self.obs, dtype = np.float32)\n",
    "        actions = self.get_actions(self.obs)\n",
    "        next_obs, reward, terminated, truncated, infos = self.envs.step(actions)\n",
    "\n",
    "        true_next_obs = next_obs.copy()\n",
    "        if terminated | truncated:\n",
    "            true_next_obs = infos[\"final_observation\"]\n",
    "\n",
    "        self.buffer.add(self.obs, actions, reward, true_next_obs, terminated)\n",
    "        self.obs = true_next_obs\n",
    "        self.step += 1\n",
    "\n",
    "        return infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives a dict of the episode length & reward & duration for the first terminated env, or `None` if no envs terminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode_data_from_infos(infos):\n",
    "    for final_info in infos.get(\"final_info\", []):\n",
    "        if final_info is not None and \"episode\" in final_info:\n",
    "            return {\"episode_length\": final_info[\"episode\"][\"l\"].item(), \n",
    "                    \"episode_reward\": final_info[\"episode\"][\"r\"].item(),\n",
    "                    \"episode_duration\": final_info[\"episode\"][\"t\"].item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.rng = np.random.default_rng(args.seed)\n",
    "        self.run_name = f\"{args.env_id}_{args.wandb_project_name}__seed{args.seed}__{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        self.envs = env\n",
    "\n",
    "        action_shape = self.envs.single_action_space.shape\n",
    "        num_actions = self.envs.single_action_space.n\n",
    "        obs_shape = self.envs.single_observation_space.shape\n",
    "\n",
    "        self.buffer = ReplayBuffer(obs_shape, action_shape, args.buffer_size, args.seed)\n",
    "\n",
    "        self.q_network = QNetwork(obs_shape, num_actions).to(device)\n",
    "        self.target_q_network = QNetwork(obs_shape, num_actions).to(device)\n",
    "        self.target_q_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = t.optim.AdamW(self.q_network.parameters(), lr = args.learning_rate)\n",
    "\n",
    "        self.agent = DQNAgent(self.envs,self.buffer, self.q_network, args.start_e, args.end_e, args.exploration_fraction, args.total_timesteps)\n",
    "\n",
    "    def prepopulate_replay_buffer(self):\n",
    "        n_steps_to_fill_buffer = self.args.buffer_size\n",
    "        self.add_to_replay_buffer(n_steps_to_fill_buffer)\n",
    "\n",
    "    def add_to_replay_buffer(self, n, verbose):\n",
    "        data = None\n",
    "        t0 = time.time()\n",
    "\n",
    "        for step in tqdm(range(n), disable = not verbose):\n",
    "            infos = self.agent.play_step()\n",
    "            new_data = get_episode_data_from_infos(infos)\n",
    "\n",
    "            if new_data is not None:\n",
    "                data = new_data\n",
    "                wandb.log(new_data, step = self.agent.step)\n",
    "\n",
    "        wandb.log({\"Samples per second\": n / (time.time() - t0)}, step = self.agent.step)\n",
    "        return data\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
